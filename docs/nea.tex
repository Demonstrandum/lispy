%  -*- coding: utf-8 -*-
%!TeX spellcheck = en-GB,en-US

\documentclass{article}
\usepackage{fancyvrb}
\usepackage{verbatim}
\usepackage[english]{babel}
\usepackage{fontenc}
\usepackage{fontspec}
\usepackage{pmboxdraw}

\begingroup
  \def\DeclareUnicodeCharacter#1{%
    \begingroup
      \lccode`\~="#1\relax
    \lowercase{\endgroup
      \global\catcode`~=\active
      \gdef~%
    }%
  }%
  \input{pmboxdrawenc.dfu}%
\endgroup

\usepackage{textcomp}
\usepackage{pmboxdraw}
\usepackage{varwidth}
\usepackage{url}
\usepackage[margin=4.00cm]{geometry}
\usepackage{pgfplots,pgfplotstable}
\usepackage{bold-extra}
\pgfplotsset{/pgf/number format/use comma,compat=newest}

\usepackage{enumerate}
\renewcommand{\labelitemi}{{\boldmath$\cdot$}}
\renewcommand{\labelitemii}{-}


\setmonofont[Scale=0.85]{Fira Code}
\newcommand{\code}[1]{\texttt{#1}}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{gensymb}
\usepackage{enumitem}
\usepackage{lipsum}
\usepackage{changepage}
\usepackage{minted}
\newenvironment{snippet}[1][python]
  {\VerbatimEnvironment
   \begin{minted}[mathescape,
     linenos,
     numbersep=5pt,
     gobble=2,
     frame=lines,
     framesep=2mm]{#1}}
  {\end{minted}}


\usepackage[
  autocite=superscript,
  backend=biber,
  style=numeric,
  citestyle=numeric,
  sorting=none,
  dateabbrev=false,
  uniquelist=false
]{biblatex}
\addbibresource{references.bib}

\usepackage{graphicx}
\usepackage{float}
\graphicspath{ {./} }

\renewenvironment{abstract}{
\begin{minipage}{0.95\textwidth}
  \rule{\textwidth}{1pt}}
  {\par\noindent\rule{\textwidth}{1pt}
\end{minipage}}
\makeatletter

\usepackage{xparse}

\NewDocumentCommand{\graph}{ m m O{17cm} } {
  \begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth,height=#3,keepaspectratio]{#1}
    \caption{#2}
  \end{figure}
}

% \newcommand{\graph}[2]{
%   \begin{figure}[H]
%     \centering
%     \includegraphics[width=\textwidth,height=17cm,keepaspectratio]{#1}
%     \caption{#2}
%   \end{figure}
% }

\newcommand{\under}[1]{\centerline{\footnotesize{#1}}}
\newcommand{\super}{\textsuperscript}
\newcommand{\numero}{N\super{\underline{o}}}
\newcommand{\etc}{{\&}c.}
\newcommand\blfootnote[1]{%
  \begingroup
  \renewcommand\thefootnote{}\footnote{#1}%
  \addtocounter{footnote}{-1}%
  \endgroup
}

\begin{document}
  \pagenumbering{gobble}

  \title{Design and Implementation of a\\LISP Dialect \& Interpreter\\
    \medskip
    \large{\textrm{AQA NEA Comp. Sci.}}}
  \author{Samuel F. D. Knutsen}
  \date{Last Compiled: \today}
  \maketitle
  \blfootnote{Beauchamps High School and Sixth Form,}
  \blfootnote{Centre \numero 16137,}
  \blfootnote{Candidate \numero 2248.}
  \clearpage

  \tableofcontents
  \clearpage
  \pagenumbering{arabic}

  \setlength{\parindent}{2em}
  \setlength{\parskip}{0.75em}

  \begin{abstract}
    \textsc{\textbf{Abstract}} ---
    Throughout this paper I will be looking at use cases, motivations,
    implementation, documentation and usage of building an interpreter for
    our new Lisp Dialect.
    \vspace{-5pt}
  \end{abstract}

\section{Analysis}
  In this section I will be investigating the usage and implementation of
  a Lisp Interpreter of my own variety, i.e. the language will be inspired
  by the conventions and harbour the basic properties of a Lisp. These
  variations upon the language are called Lisp Dialects, one of which will be
  my own, which I will be implementing, and outlining in this paper.

  LISP -- is a style of programming language, and stands for
  \textsc{\underline{LIS}t \underline{P}rocessor}.\\
  because of the fact that the entire language itself is simply constructed from
  data, which are just lists of \emph{datum}, but more on that later.

  Designing and implementing a language are two distinct steps, but I need
  to consider both while working on each of the steps. That's to say, that
  when designing the language, I need to consider what would be realistic in
  implementing the language afterwards. The features of a language are
  limited to what I will be able to actually write the code for, as well as not
  allowing for ambiguity in the language.
  In the second stage, when implementing it, I also
  need to stay true to our original design, and not stray from what I
  originally intended (with the exception of allowing expansion upon the language),
  unless I had set unrealistic design goals.

  \subsection{Motivation}
    Programming languages are not only naturally very important for programming
    and programmers themselves,
    but also a very fascinating topic, bridging the gap between
    what is human language, and what is a computer language.

    % For the reason of interest alone, I already wanted to make a programming
    % language.  I have made programming languages a few times before, one of them
    % another interpreter meant to resemble assembly code, meant for getting
    % used to the concepts of Assembly and how a computer works, by also creating
    % a website to visualise the internals of a computer (website: fam.knutsen.co).
    % However the language serves not much purpose other than for education.

    My motivations for making a Lisp interpreter, however, are many.
    Lisp, for one, is quite a powerful language, and I mean that in the sense
    of how much you can do and manipulate the language and your program themselves.
    The major advantage of Lisp is indeed that it can be manipulated the very
    same way data can be manipulated, as the language is data itself.

    Lisp macros are really what sets Lisp apart. If I asked you to implement
    a while loop statement or an if statement in Python, could you do it?
    If you tried, you'd end up with an ugly mess of lambdas or perhaps even pieces
    of your program written inside strings that get parsed \emph{after} run time, and
    such.  In Lisp such a matter is trivial, thanks to its powerful macro system
    that is capable of building parts of the program in the program itself.

    \clearpage

    Speaking of the \code{while} loop, it is indeed very often implemented
    in the Lisp language itself, as opposed to being built into the interpreter.
    Here is my example, which is quite trivial, and will become more
    understandable, after having read this paper through:

    \begin{minted}{Lisp}
      (define macro (while condition body)
        (iterate
          (if (eval condition)
            (eval body)
            break)))
    \end{minted}


    Another motivation is the potential that I may implement some features
    into the language that other Lisp dialects don't have, in addition, I can
    also control certain features that I don't want in the language, that
    other Lisp dialects almost almost religiously include (for example
    confusing naming on \code{car}, \code{cdr} and such, when \code{head}
    and \code{tail} (or \code{rest}) would suffice).

  \subsection{Research}
    To begin this project, a certain amount of research needs to be covered.
    First we need to familiarise ourselves with the general concept of what
    defines and encompasses a Lisp and what I'd like my dialect to be
    inspired by.

    \subsubsection{What makes a Lisp?}
    Lisp is a \emph{family} of scripting/programming languages belonging typically
    to the declarative/functional paradigm of programming languages. It is
    characterised by its heavy use of parentheses (`\code(' and `\code)').
    The reason for this, is because the entire
    syntax itself is also a data structure\autocite{milestones}.
    More on the specifics on the syntax in the design section.

    \subsubsection{Making our language executable}
    Essentially, this is how to make a programming language be evaluated and
    how it can become executable on a computer, by a computer.

    Almost every implementation of the most common programming languages start
    with what is known as a `lexer' or `tokeniser' that performs lexical analysis.
    As one might gather from the name, this stage in the
    implementation tokenises the program.
    Every program starts of as just a long string of characters stored in a file, these
    individual characters aren't useful, we want to gather them up in to groups
    of characters, e.g. the program ``\code{print("Hello World")}'' is understood by
    the computer as just a string of characters:
    \code{['p', 'r', 'i', 'n', 't', '(', '"', ...]}, this is not useful.
    We want to group the individual components of the language together.
    As such:

    \noindent\code{[[IDENTIFIER, 'print'], [L\_PAREN, '('], [STRING, 'Hello
    World'], [R\_PAREN, ')']]}

    After this we proceed to the parsing stage. This essentially forms an
    abstract syntax tree from the tokens. This let's us define some sort of
    separation and concept of nesting between statements, grouping them together
    in a logical order.\autocite{dragon}
    e.g.

    \[ \code{"2 + 3"} \Rightarrow \code{[[NUMERIC, '2'], [OP, '+'], [NUMERIC,
'3']]}  \]
    \centerline{becomes}
    \begin{center}
      \begin{tabular}{l}
        \begin{BVerbatim}
         ...
          │
          ├───BINARY_OP::PLUS
          │    │
          │    ├───INTEGER<2>
          │    └───INTEGER<3>
         ...
       \end{BVerbatim}
      \end{tabular}
      As a branch on a bigger syntax tree.
    \end{center}


    \centerline{Another example:}
    \[ \code{"-4 * 3"} \Rightarrow \code{[[OP, '-'], [NUMERIC, '4'], [OP, '*'],
[NUMERIC, '3']]}  \]
    \begin{center}
      \begin{tabular}{l}
        \begin{BVerbatim}
         ...
          │
          ├───BINARY_OP::MUL
          │    │
          │    ├───UNARY_OP::NEG
          │    │    └───INTEGER<4>
          │    └───INTEGER<3>
         ...
       \end{BVerbatim}
      \end{tabular}
    \end{center}


    Now that we have our syntax tree, I will have to implement some sort of a
    visitor/walker, in order to execute and evaluate our instructions. Usually
    we would start with the leaves of the tree and work ourself up,
    eventually evaluating the whole expression, having evaluated the
    sub-expressions first.

    Luckily for me, I needn't consider operator precedence, as `operators' are
    just function names which can be called, which forces prefix/polish notation
    to be used, where precedence is implied by the order of the operators and
    their bracketing. Another
    pro of parsing Lisps is that the Abstract Syntax Tree (`AST' from here on)
    already has a lot of similarity to the language itself, over all parsing is
    much easier than it would be for some of today's more `human oriented',
    interpreted programming languages.


\section{Design of the language}
  There are many ways of implementing a Lisp, the earliest Lisps had a very
  limited amount of key/reserved words (about 4), which from the early versions
  of MacLisp and Common Lisp has increased to about a minimum of about 9 or 10.

  One of the most important and powerful features of Lisp is that its
  syntax is practically its own syntax tree.  These nested lists that the Lisp
  grammar is composed from are called s-expressions\autocite{sexp}, meaning
  ``symbolic-expressions''.
  \subsection{Symbolic Expressions}
    In many (but not all) Lisp implementations and dialects, one of the most
    fundamental datatypes is the cons cell.  The cons cell is a way of constructing
    trees, and thus lists alike.

    Lisp ASTs are essentially linked-lists, capable of containing pointers to
    other nested lists (i.e. a `tree').
    Take the list \code{(x y z)} for example, to represent this in the form
    of cons cells, we need to consider the fundamental cons function. The cons
    function takes in a fundamental datum as first argument (can also be a pointer
    to another list) and a pointer
    to the next cons cell, and to end the list, simply a NULL pointer (an empty
    list or NIL is often used in the place of NULL).

    Hence, \code{(x y z)} becomes \code{(cons x (cons y (cons z NULL)))}.\\
    But this way of writing is verbose, so the preferred notation is the
    dot-pair notation,\\
    so, \code{(cons x (cons y (cons z NULL)))} is written as \code{(x . (y . (z . NULL)))}

    \clearpage
    A linked-list/n-ary tree as such can implemented very plainly in C:

    \begin{minted}[mathescape,
                   linenos,
                   numbersep=5pt,
                   gobble=2,
                   framesep=2mm]{c}
    struct cons_cell {
      atom data;
      struct cons_cell *next;
    };

    typedef struct cons_cell *cons;

    // Thus, (x y z) will be:

    atom x = Symbol('x');
    atom y = Symbol('y');
    atom z = Symbol('z');

    cons make_cons() {
      cons parent = malloc(sizeof(struct cons_cell));
      parent->next = NULL;

      return parent;
    }

    /*  I could simply write a quick little function for what
     *  I'm about to do, but I'll write out the construction of the
     *  cons list explicitly, such that it's more obvious what we're doing.
     */
    cons list = make_cons();
    list->data = x;
    list->next = make_cons();
    list->next->data = y;
    list->next->next = make_cons();
    list->next->next->data = z;
    list->next->next->next = NULL;  // Same as (x . (y . (z . NULL)))
    \end{minted}

    Which can be visualised as:
    \graph{cons-cells}{Visual representation of cons cells (i.e. a linked list)}


    However, I will not be implementing my s-exps in this manner, nor
    will my Lisp dialect even contain the function \code{cons}
    (although \code{cons} can be very easily implemented as a macro).
    This is  because I will not be writing this in C, but rather in a higher
    level language (Python $>$3.7).  I'll be writing this in Python as it is
    what my school has available and it is what is taught there.

  \subsection{Syntax}
    Lisp consists of lists, and lists within lists,
    as I discussed earlier. Every list is denoted by a set of
    parentheses where the first element is generally a function name, and the
    following being its arguments. Some lists get expanded from their macro at
    the pre-compilation/preprocessing stage,
    and eventually all lists get evaluated as a
    function call. One might wonder how one makes a normal list, if Lisp treats
    all lists as function calls. Lisp has a somewhat unique feature, and that is
    the `unevaluate'/`quote' ``operator'' (if you will) which is denoted by
    putting
    a `\code{'}' (single single-quotation mark)
    in front of any expression, to stop the interpreter from evaluating it.
    So, your standard list or array is denoted:

    \begin{center}
      \code{'(a b c)}\\
      And when we want to return it back to its former unquoted self,\\
      we call the \code{eval} function on it:\\
      \code{(eval '(a b c))} is the same as \code{(a b c)}
    \end{center}

    Here we see that list items are separated by spaces, rather than the more
    common comma (\code{,}) in other languages, making white-space significant to some
    degree. However, other than that, white-space is quite insignificant and
    the language has no need for things such as terminators (terminators are
    semi-colons in C and a new line in Python for example), and that's because
    everything is neatly wrapped up in parentheses and therefore every statement
    is automatically separated from each other, anyway.

    \subsubsection{Polish Notation / Prefix notation}
    Lisp could be said to be a language using Polish notation, that's to say
    that the language places its `operators' (which we'll discuss later) as prefixes,
    meaning they're put in front of operands.  Traditionally operators are placed in
    between their operands, this is called `infix' notation ($1 + 2$ for example).

    Prefix notation has some advantages and disadvantages, really, the only
    disadvantage to prefix notation is the fact that it may be less readable.
    The \emph{sole} reason for the fact that it is less readable is simply
    that we are not used to it, because we're used to infix notation, instead.
    We could just as easily be using prefix notation, but things didn't turn out
    that way, but that doesn't prevent us from learning and quickly getting used
    to it.

    On the other hand, the advantages really do make up for the disadvantages.
    One major advantage is the fact that precedence is \emph{implied}, if and only if
    the exact amount of operands an operator can have are two; when that is true
    there is absolutely no need for things such as parentheses. However, Lisp
    \emph{does} make heavy use of parentheses and thus suffers from another
    formidable advantage, `n-arity' of operators. N-arity, Polyadic or Variadic
    functions/operators are simply operators that can take any number of operands.
    How would you add up five numbers together using infix notation?
    You'd have to use the operator four times over (e.g. $1 + 2 + 3 + 4 + 5$
    which bracketed would become $1 + (2 + (3 + (4 + 5)))$ (because really infix
    operators can only take two arguments)) as opposed to in Lisp, where you only
    need to use the $+$ operator once, (e.g. $(+\ 1\ 2\ 3\ 4\ 5)$, far fewer
    characters, use of operator was only once and no implicit brackets).

    The fact that Lisp uses prefix notation was not necessarily a choice for
    the language per se, but rather a consequence of the languages structure and
    a symptom of the language's aim to, itself, \emph{be data}.

    \subsubsection{Unique Data-types}
      Most Lisp dialects have about 7 fundamental datatypes:
      \begin{enumerate}
        \item \textsc{Integers}
        \item \textsc{Floats}
        \item \textsc{Cons}
        \item \textsc{Symbols} (including \textsc{Keywords})
        \item \textsc{Strings}
        \item \textsc{Functions}
        \item \textsc{Null}
      \end{enumerate}

      However as mentioned before, because of the way I'll be implementing this
      dialect, I will be excluding \code{cons} as a part of the language and will
      also be introducing a new one, which I'm calling the \textsc{Atom}.

      My new list of datatypes which will be implemented in my dialect is as
      following:

      \begin{itemize}
        \item \textsc{Numerics} (an umbrella for integers and floats)
        \item \textsc{Symbols}
        \item \textsc{Atoms}
        \item \textsc{Strings}
        \item \textsc{Definitions} (impure functions)
        \item \textsc{Nil}
      \end{itemize}

      My atoms are not at all your standard Lisp atom, in most Lisp contexts
      `atom' just means a fundamental datum, but this is not what I'll be
      calling an atom.

      Atoms are a useful construct in my experience. The Ruby programming language
      has them, and calls the `Symbols', however that means something totally
      different in the context of a Lisp, the Erlang and Elixir programming
      languages have them, and many Lisps have something that heavily resembles
      atoms (both syntactically and practically) called `Keywords', but my `Atom'
      is slightly different.

      %% Extract from a file comment I made
      Atoms simply exist for their name.
      No atoms ever need to be created/initialised (manually) as
      one could simply imagine that all
      possible atoms already exist at once.

      When an atom is first used, it is assigned
      a location in memory based on some hash, and
      any future use of that same atom
      references the same exact location in memory.

      Atoms are not strings, you can't change anything
      about them, to do that, simply use another atom.
      Two separate identical strings will take up and be
      declared in different locations in memory, this is
      not true for atoms. Atoms will always use the same
      location in memory, there is no reason for them to
      do otherwise, (unlike with strings)

      Atoms must start with a colon, then an identifier
      string, just like symbols (but with a colon in front).
      One may think of atoms as being used the same way
      as \code{enums} are, existing only for the purpose of
      having a way to identify something.\\

      \centerline{e.g. \code{:this-is-an-atom}}

    \subsubsection{Operators}
      Technically in Lisp, any function could be called an operator.
      It is also true that all the operators we traditionally call operators
      are also operators in Lisp (i.e. \code{+, -, *, /, \%,} \etc).
      Hence, transitively, all (traditional) operators are functions!

      The fact that all operators are just functions, I see as a hugh advantage
      to the language, because, not only does that allow the programmer to
      extend the meaning of an operator, but it also allows them to create their
      own operators just as easily as defining any function. Another consequence
      of this that we may use operators in our function names / symbols.
      Just as \code{+} is a function, you could have a function named \code{++}
      or \code{***}, and that can be used with alphanumeric characters too, to
      have a function \code{*hello*} for example, and even \code{hello-world},
      which is a common mistake amongst beginner programers, trying to use hyphens
      in variables names, but the language (most likely C or Python, \etc),
      understands \code{hello-world} as \code{hello} \emph{minus} \code{world},
      as opposed to what the programmer \emph{actually} meant. Perhaps another
      advantage of Lisp\ldots

    \clearpage
    \subsubsection{Key/Reserved Words \& Symbols}
      Every language has a certain amount of reserved words and symbols that
      actually form, comprise and compose the syntax of a language, some
      languages will allow you to redefine these key words, but most won't, as
      it would essentially break the language.  Basic examples of these key
      words, in Python for example, are words such as: \code{return}, \code{while}
      \code{def} and \code{True} (also including operators such as \code{=}).

      My language should have a few defined reserved words, but my aim is to keep
      the number of them as low as possible, because, like in many Lisps, many
      things can be defined as macros in a standard/prelude library anyway.
      Some words that I plan to reserve are the following:

      \begin{itemize}
        \item Yield statement, similar to the return-statement, \code{(yield ...)}.
        \item Unevaluator, this is the single single-quote, \code{'some-name},
              \code{'(some list of symbols)}.
        \item Nil, this is the only kind of its type,\\
              and will be entirely reserved as its own lexeme, \code{nil}.
        \item All symbols such as \code{(} and \code{)}, and \code{"}, as they
              exist to describe other constructs. (i.e. lists and strings)
      \end{itemize}

      Other than that, the semi-colons (\code{;}) will describe EON comments,
      Atoms and Symbols will be matched through certain expressions, and
      anything else will be completely ignored by the language.

      Other items, that are not understood by the Lexer and Parser, but are
      still technically part of the list of reserved words are the internal
      macros, which will be identified and dealt with at the evaluation stage
      are called `internal macros`, because their behaviour is technically like
      macros, but they are built internally into the interpreter.

    \subsubsection{Internal Macros}
      Internal macros in a language are also not usually allowed to be reassigned
      in most languages, however a surprisingly large amount of Lisp dialects
      and implementations do allow this, however I will not be allowing this
      myself.


      \clearpage
      My initial list of internal macros will be as following, (but I am certain
      it will expand when I actually implement the language):

      \begin{itemize}
        \item \code{define} -- Probably the most important macro, it takes in a
                               name, arguments for a function and a function body,
                               and creates a
                               function in the current scope with that name.
                               Essentially the equivalent of \code{def} in
                               languages such as Ruby, Python, Scala, \etc

        \item \code{lambda} -- Same as define, but the function is nameless.
                               This is also called an anonymous function.

        \item \code{let} -- Would bind a value to a symbol, say $x = 3$, in Lisp
                            we'd write \code{(let (x 3))}.

        \item \code{print} -- Print the operands of the print statement to \code{STDOUT}.
        \item \code{if} -- \code{if} is a three way statement, the first operand is the
                           condition, the second is the consequence, the third
                           is the alternative,
                           if-\underline{this}-then-\underline{that}-else-\underline{that}.

        \item \code{<} -- Check if each operand is less than the next.
        \item \code{>} -- Check if each operand is greater than the next.
        \item \code{=} -- Check if all operands are the equal to each other.
        \item \code{/=} -- Check if at least one operand is not the same as the rest.

        \item \code{type} -- Returns the type of its evaluated operand.

        \item \code{list} -- Creates an unevaluated list, but all the operands
                             get evaluated at the construction of the list.



        \item \code{+} -- Addition macro will add all its operands.
        \item \code{-} -- Subtraction macro will subtract all its operands from each other.
        \item \code{*} -- Multiplication macro will multiply all its operands.
        \item \code{/} -- Division macro will divide all its operands  by each other.
        \item \code{\%} -- Mod macro will give the remainder of the division of
                           all its operands.

        \item \code{eval} -- Takes an unevaluated list or a string, and evaluates
                             it as normal code, a very important macro in any
                             Lisp dialect.
      \end{itemize}




  \subsection{Grammar}
    Any Lisp's grammar is naturally very similar, and for the most part quite
    simple, but has indeed steadily been increasing in complexity over the years.
    The Original Lisp had a very simple grammar and syntax, and had only the
    symbols \code{DEFINE}, \code{LAMBDA}, \code{LABEL}, \code{COND},
    \code{COMBINE}, \code{FIRST}, \code{REST}, \code{NULL}, \code{ATOM},
    \code{EQ}, \code{NIL}, \code{T}, and \code{QUOTE} predefined\autocite{sexp}.
    All symbol names were converted to upper-case and thus
    symbol names became case insensitive in early Lisps, this is
    still a tradition upheld in many Lisps today (because it's said to be less
    error prone), however this is not a tradition I will be upholding.

    Many modern Lisps have not only introduced the shortened quote syntax\\
    (\code{'...} as a shorting of \code{(quote ...)}), but have also introduced
    another form of quote, the `quasiquote'.

    The quasiquote works just as the normal quote but with added functionality.
    It can ``unquote'' selective operands through use of the \code{(unquote ...)}
    macro inside of the \code{(quasiquote ...)} macro. This essentially means
    you can choose to evaluate certain operands, unlike quote, which keeps all
    operands unevaluated until you choose to evaluate them individually after
    its initial construction, when accessing the unquoted list.

    In modern Lisps, some further syntactic sugar has been introduced
    for the quasiquote:\\

    \begin{tabular}{lll}
        \code{(quasiquote ...)}       & has been aliased to& \code{`...} (a backquote/backtick)\\
        \code{(unquote datum)}        & has been aliased to& \code{,datum}\\
        \code{(unquote-splicing data)}& has been aliased to& \code{,@data}
    \end{tabular}

    {\small(Where \code{unquote-splicing} unquotes an entire list and merges it with
    the parent list)}

    Some examples of behaviour include:
    \begin{minted}[escapeinside=||,mathescape=true]{Lisp}
    `(1 2  (+ 3 4) 5)      |$\Rightarrow$|    '(1 2 (+ 3 4) 5)
    `(1 2 ,(+ 3 4) 5)      |$\Rightarrow$|    '(1 2 7 5)
    `(1 2 ,@(list 3 4) 5)  |$\Rightarrow$|    '(1 2 3 4 5)
    \end{minted}

    Lisp syntax is always expanding, and the sharp-sign (\code{\#}), is actually
    a syntax element, purely meant for expanding the syntax itself, for example,
    in Common Lisp, \code{\#{\char`\\}c} is a character, \code{\#(...)} is a
    \emph{vector} and \code{\#xff} is the hexadecimal for $255_{10}$. At this point
    I personally think the syntax is getting out of hand, and I doubt I'll be
    implementing this in my Lisp dialect.

    \subsubsection{Generating a Token Stream} \label{lexing}
      The lexical analyser is responsible for reading through our program string
      and splitting up the program in to sensible tokens representing a single
      datum, that comprises and represents something in the languages syntax.
      Take for example the program:\\``\code{(+ 12 (* 3 4))}'', our brains have
      already started tokenising and parsing the text, as soon as we look at it,
      and we can identify quickly some basic components, brackets, operators,
      numerics and spaces, but, for a computer these are all characters of no
      particular separation. A lexers job is to separate them in to parts, such
      as say\\\code{List [ Sym +, Num 12, List [ Sym *, Num 3, Num, 4 ] ]}, and
      ensure we dont get any non-sense tokens such as `\code{(+}' or `\code{* 3}'.

      \clearpage

      Very often, when parsing regular languages, a lexical analyser may use
      simple regualar expressions to parse the language, say for example, a
      very primitive numeric matcher (for ints and floats), may look something
      like:

      \centerline{\code{/[0-9]+(\char`\\.[0-9]+)?/}}

      In pseudo-code, a basic lexer may look something like:

      \begin{minted}[linenos]{ruby}
      TokenStream = List

      lexer :: String -> TokenStream
      lexer (program) =>
        char_pointer = 0
        stream = new TokenStream

        partial = program[0..]

        while partial[0] != EOF =>
          condition if
            | (partial[0] == '(') =>
                stream.push (new Token L_PAREN)
                char_pointer = char_pointer + 1
            | (partial[0] == ')') =>
                stream.push (new Token R_PAREN)
                char_pointer = char_pointer + 1
            | (partial[0..2] == 'nil') =>
                stream.push (new Token NIL)
                char_pointer = char_pointer + 3
            | (match /[0-9]+(\.[0-9]+)?/ partial[0..]) =>
                stream.push (new Token NUMERIC matched)
                char_pointer = char_pointer + length(matched)
            | (match /[a-zA-Z_]+[a-zA-Z_0-9]*/ partial[0..]) =>
                stream.push (new Token IDENTIFIER matched)
                char_pointer = char_pointer + length(matched)
            | otherwise =>
                char_pointer = char_pointer + 1

          partial = program[char_pointer..]

        return stream
      \end{minted}

      \graph{lexer-flow}{The lexer also represented by a flow-chart.}

      This basic function \code{lexer} does most of the works, it takes in a
      \code{String} (the program string itself) and gives a \code{TokenStream}
      (which is really just an alias for a list). The lexer initialises some
      variables, very notably, the \code{char\char`_pointer}, which very simply keeps
      a track of how many characters through the program code we are. Then the
      \code{partial} variable is just a section of the program string, it begins
      at the location of \code{char\char`_pointer} and all the way to the end
      of the string.\\

      A while-loop will run all the way until we hit a null (\code{\char`\\0})
      EOF string terminator, and checks various conditions as well as incrementing
      the \code{char\char`_pointer} and updating the \code{partial} variable.
      The conditions check whether we've matched a certain token out of the beginning
      of the \code{partial} string, if so, we push a new \code{Token} to the
      \code{stream} list, with the appropriate type and matched string supplied
      with it.

    \subsubsection{Generating an Abstract Syntax Tree}\label{ast}
      Now that we have our list of tokens, the next step is to take those tokens
      and reconstruct the program as a tree.  The tree introduces important
      concepts back into the structure of the program.  Things such as nesting
      and precedence are clearly represented through the use of a tree.  A tree
      will also help us give appropriate scoping within certain nests at the
      evaluation stage.

      The parsing pattern we'll be implementing for generating our tree will
      likely be a bottom-up parser (or a shift-reduce parser). A shift-reduce
      parser does exactly what its name says it does:
      it shifts tokens off the token stack, and parses the \emph{leaves} of the
      tree, before it generates their super-branches (hence the name bottom-up).
      It is often implemented recursively (as with many parsers) and is
      generally a lot more effective than a top-down parser, which requires a
      lot of guess-work.

      A general implementation of such a parser written in pseudo-code,
      could look something this:
      \clearpage
      \begin{minted}[linenos]{ruby}
# -- Get a our stream of tokens by calling the lexer
stream : TokenStream = Lexer::lex PROGRAM_STRING

# -- Define various AST datatypes
DataType Tree =>
  self.children = []

DataType Call (values) =>
  self.values = values
  get self.operator =>
    return head (self.values)
  get self.operands =>
    return tail (self.values)

# --- Generalised datatype for an atomic AST datum
Abstract DataType Atomic (value) =>
  self.value = value

DataType Numeric inherits Atomic  # Atomic types represent a single datum.
DataType Symbol  inherits Atomic

# -- Actually implement the parser
parse :: TokenStream -> Tree  # (Type annotation)
parse (stream) =>
  tree = new Tree # Create an empty tree-root
  until empty? stream =>
    tree.children.push (atomic (stream)) # Deals with parsing individual datum
    stream.shift # Now shift off of the TokenStream stack
  return tree # Return the super-tree we've built up.

atomic :: TokenStream -> (Call | Atomic)
atomic (stream) =>
  condition if =>
    | (stream[0].type is L_PAREN) =>
        call = new Call []  # An open left-parentheses means a function call.
        until stream[0] is R_PAREN => # Serch for a maching right-parentheses.
          stream.shift # shift off L_PAREN from stack (initially).
          call.values.push (atomic (stream))   # Push and recursively call
        stream.shift # Shift the R_PAREN off   # atomic on shifted stack.
        return call
    | (stream[0].type is NUMERIC) => return new Numeric (stream[0].string)
    | (stream[0].type is IDENTIFIER) => return new Symbol (stream[0].string)
    | otherwise =>
        Throw (UnknownTokenType,
          "Token type" ++ stream[0].type ++ "is unknown.")
        # If our lexer was implemented properly, we wouldn't have to throw
        # an error, so we'll hopefully never reach the `Throw`.

      \end{minted}
      \clearpage

      The above code, may seem weird at first glance, so let's go through
      both of the functions defined above.

      \code{parse} --- The function \code{parse}, takes in a stream variable of
      type \code{TokenStream} and returns a tree of type \code{Tree} (which is
      basically a wrapper for a list).  Somewhat ironically, it doesn't implement
      any of the parsing algorithm itself, but rather calls a delegated
      function (\code{atomic}) to handle the parsing of atomic data. \code{parse}
      will run a loop, which runs until the stream of tokens has been completely
      emptied. In the loop body, we call the delegated \code{atomic} function on
      the stream, whose return value will get pushed as a child/branch onto the
      parent/root-tree. After that we shift one token off the stack and repeat
      until all tokens have been shifted off, (note: \code{atomic} may shift
      tokens off the stack as well, when parsing sub-expressions for a bigger
      expression).

      \code{atomic} --- The \code{atomic} function takes a (probably incomplete,
      due to shifting) token-\code{stream} variable of
      type \code{TokenStream} and returns either a \code{Call} node or a derived
      \code{Atomic} node such as \code{Numeric} or \code{Symbol} (i.e.
      \code{(Call | Atomic)}). \code{atomic}, is just one big conditional, that
      operates depending on which token happens to be on top of the stack at that
      point in time.  The first condition checks whether we have a function call
      being opened/started in our code (i.e. we've seen a left-parenthesis).

      \begin{adjustwidth}{1cm}{}
      \indent \indent If we do spot an \code{L\_PAREN}
      atop of our stack, we do indeed have a function call,
      and hence we must continue shifting off the stack until we reach a closing
      right-parenthesis.  But, remember, being a bottom-up parser, we must first
      parse all sub-expressions, so we recursively call \code{atomic}, to deal
      with all the expressions contained within the parentheses. Doing this, we
      also have implemented and permitted nesting into our language, by allowing
      other function calls to be parsed within parent function calls. When we've
      finally reached that corresponding right-parenthesis, and have pushed all
      our parsed sub-expression to the function call, we return the \code{Call}
      node and we end up back in the \code{parse} function, where the \code{call}
      node is pushed to the root-tree (or it may indeed be pushed to another
      parent call-node).
      \end{adjustwidth}

      \clearpage

      \subsubsection{Macro Expansion Phase}
      After we've generated the most basic syntax tree, we should start the macro
      expansion phase.  Macros, you may be familiar with from other languages
      (especially compiled languages), they're part of the preprocessor stage,
      and thus do not exist at evaluation time.

      Let's demonstrate through an example. Take the macro, \emph{and} the function:

      \begin{minted}[escapeinside=||,mathescape=true]{Lisp}
        (define macro    (add-3-macro x) (+ x 3))
        (define function (add-3-func  x) (+ x 3))

        (print (add-3-macro 2) "\n") |$\Rightarrow$| 5
        (print (add-3-func  2) "\n") |$\Rightarrow$| 5
      \end{minted}

      Both \emph{eventually} end up evaluating to \code{5}, but did so by different
      methods. \code{add-3-func} had it's own Symbol Table made, that symbol table
      was pushed to the call stack, and the symbol \code{x} was bound in it.
      \code{x} was bound to the value \code{2} when the function was called, the
      function returned the evaluation of its body, with it's own bound symbols
      and then had it's symbol table wiped clean, and popped of the interpreter's
      call stack. Wew! All that for something that could have just been written
      as \code{(+ 2 3)}\ldots Well, that's exactly what that macro did!

      The macro, is essentially just text replacement (although what the macro
      accomplishes is actually done through manipulating the syntax tree).
      So, when the macro-expander sees your making a call with a macro-name as the
      caller, it essentially copy-pastes the arguments into the macro body, and
      replaces all instances of \code{x} with \code{2} (in this example).

      So essentially,
      \begin{minted}[escapeinside=||,mathescape=true]{Lisp}
        (print (add-3-macro 2) "\n") |$\textrm{expands to:}$| (print (+ 2 3) "\n")
      \end{minted}

  \subsection{Interpreter}
    Finally, to what's probably the most important part of the implementation.
    The interpreter or \emph{evaluator} is what will be doing the computation,
    by traversing the syntax tree that we've just generated.

    The interpreter will essentially consist of three important parts/aspects.
    \begin{enumerate}
      \item Symbol Tables, and usage of them in Scoping.
      \item Defining internal macros/functions.
      \item Running through the sub-trees of the AST recursively and evaluating
      appropriately certain expressions, by first evaluating the leaves of the
      tree, slightly similar to how we parsed the program.
    \end{enumerate}

    \graph{example_interp}{Here is an example of how a typical fully functional
    REPL-based Lisp interpreter might function.}

    \clearpage

    \subsubsection{Scoping \& Binding Symbols in Symbol Tables}
    Let's start off by defining various symbol-table stacks (such as call-stacks
    and \emph{frozen}-stacks and a list of current, parent scopes)

\begin{minted}[linenos]{ruby}
DataType SymbolTable (scope_id) =>
  self.id = scope_id # A unique ID for each scope.
  self.local = %{} # A hash-table of all local variables to this scope.
  self.frozen = :false # Whether the symbol table can be modified
                       # (lambdas will have frozen super-scopes).
  self.bind (symbol, value) =>
    self.local[symbol] = value
  self.clean =>
    self.local = %{} # Empty the local variables.

# --- Define all the scope stack we need to keep track of:

ALL_SCOPES    = new Stack of SymbolTable
PARENT_SCOPES = new Stack of Integer  # Integer IDs (i.e. unique scope IDs)

CALL_STACK    = new Stack of SymbolTable
GLOBAL_FROZEN = new Stack of SymbolTable where .frozen <- :true
# ---

# A type for function definitions.
DataType FuncDef (scope, name, args, subtree) =>
  self.name = name
  self.subtree = subtree
  self.scope = scope
  self.table = find(scope, :id, ALL_SCOPES)
  self.args = args

  self.call (operands) =>  # What happens when we call a
    PARENT_SCOPES.push (self.scope) #    user defined function.
    CALL_STACK.push (self.table) # Push appropriate scopes.
    for i in length (self.args) =>
      self.table.bind(self.args[i], operands[i]) # Bind arguments,
                               # to the call stack's latest scope.
    evaluate(self.subtree) # Finally evaluate the body.

    PARENT_SCOPES.pop
    CALL_STACK.pop # Pop the symbol-table off again.
    self.table.clean # Get rid of old bindings.
\end{minted}
      This outlines the tools we'd be using, for dealing with scoping things in
      our language.
      \clearpage

    \subsubsection{Dealing with Internal Macros}
      Internal macros are really important. They provide a way to interface with
      the computer directly (or in this case, it'd be the implementation
      language (i.e. Python)), which introduces very useful functionality into
      the language. Take for example, the add \code{`+'} function/internal macro.
      It is basically required that this is not implemented in the language we're
      making, itself, as it is one of the most basic operations we can perform,
      and there'd be no way of implementing it without direct help from our
      host language. Others include \code{define} and \code{-} and so on.
      I've already run through numerous macros earlier in this paper.

      Say for example, our interpreter encounters a symbol within our defined
      list internal macros, perhaps layed out as a hash-map like so:
      \begin{minted}[escapeinside=||,mathescape=true]{ruby}
        INTERNALS = {
          :define => __DEFINITION_MACRO__,
          :+      => __ADDITION_MACRO__,
          :-      => __SUBTRACTION_MACRO__,

          |$\vdots$|

          etc.
        }
      \end{minted}

      where \code{\_\_DEFINITION\_MACRO\_\_} and such are functions that deal with
      the computation of that internal macro. So in the evaluation function,
      we'd deal with it something like this.
      \begin{minted}[mathescape]{ruby}
        # Inside the evaluation function:
        evaluate :: (Call | Atomic) -> Anything
        evaluate (node) =>
          # ...
          if typeof (node) is Symbol =>
            if node.name is in INTERNALS =>
              return INTERNALS[node.name]
            # Otherwise, look in the Symbol Tables
            return lookup_symbol (PARENT_SCOPES, node.name)
          # ...
          if typeof (node) is Call =>
            caller = evaluate (node.caller)
            if typeof (caller) is FuncDef =>
              return caller.call (caller.operands)
            if caller is INTERNAL_DEFINITION =>
              return caller (node)
            else =>
              Throw (UncallableCallerError, "Can't make call to this type...")
          # ... etc.
      \end{minted}
      \clearpage
      What would an internal representation of one of those macros look like?
      Let's take the \code{+} internal macro, perhaps it would look something
      like this (in pseudo-code):
      \begin{minted}{ruby}
        __ADDITION_MACRO__ :: Call -> Atomic
        __ADDITION_MACRO__ (call_node) =>
          args = map(evaluate, call_node.operands) # Evaluate every operand first.
          sum = 0
          for arg in args =>
            if typeof (arg) is not Numeric =>
              Throw (TypeError, arg ++ " is not a Numeric type.")
            sum = sum + arg
          return sum
          # An internal iterative solution is much more efficient
          #   than a more idiomatic recursive solution.
      \end{minted}
      Really, theres nothing fancy going on here, it is simply a way to build
      the most basic components of the language, and interface with features
      (such as basic addition) from the implementation language.
    \subsubsection{Interpretation Pattern}
      As briefly seen above, there are two basic functions I plan to be
      implementing.  A \code{visit} function and the very critical \code{evaluate}
      function.

      Let's start by describing the \code{visit} function. It's a simple to
      implement, all it needs to do is visit each root-branch of the \code{Tree}
      and evaluate each branch (which will subsequently evaluate all its
      subtrees recursively) in a simple loop. It may be implemented as such
      in pseudocode:

      \begin{minted}[]{ruby}
        visit :: Tree -> Nothing
        visit (AST) =>
          for child in AST =>
            evaluate (child)
          # The most basic idea of the visitor, although it may
          #   do error handling an other things as well.
      \end{minted}

      Naturally, we must have an implementation of the evaluate function too.
      We had partially implemented the \code{evaluate} function above, but let's
      now complete that implementation.

      \clearpage

      \begin{minted}[linenos]{ruby}
evaluate :: (Call | Atomic) -> Anything
evaluate (node) =>
  case typeof(node)
  when Numeric =>
    return literal_eval (node.value) # Doesn't really get evaluated, per se,
                                     # as its really an 'atomic' datum.
  when String =>
    return node # Already the most basic datatype.
  when Atom =>
    # if the atom already exists in memory, just return it.
    if node.value is in ATOMS_HASHMAP =>
      return ATOMS_HASHMAP[node.value]
    # Otherwise, add it to the hashmap, giving it a unique location
    #   in memory, which will be referenced whenever the same atom is
    #   used again.
    ATOMS_HASHMAP[node.value] = new Atomise(node.value)
    return ATOMS_HASHMAP[node.value]
  when Symbol =>
    if node.name is in INTERNALS =>
      return INTERNALS[node.name]
    return lookup_symbol (PARENT_SCOPES, node.name)
    #      ^^^^^^^^^^^^^ Simple to implement, obvious what it does.
  when Call =>
    caller = evaluate (node.caller)
    if typeof (caller) is FuncDef =>
      return caller.call (caller.operands) # Make call to user defined function.
    if caller is INTERNAL_DEFINITION =>
      return caller (node)
    else =>
      Throw (UncallableCallerError, "Can't make call to this type...")

  default =>
    Throw (UnknownTreeNode, "I do not recognise the type of data you've
                             passed to be evaluated.")
      \end{minted}

  \subsubsection{Including a REPL} \label{repl}
    It'd be nice with a REPL as well. Quite simply a REPL is a
    READ-EVAL-PRINT-LOOP, and its implementation is in its name.
    \begin{minted}{Lisp}
      (loop (print (eval (read))))  ;; Just one line, implements the whole thing.
    \end{minted}
    This is quite a na\"{i}ve implementation, but it still works just fine,
    all we need to do, is execute the Lisp code using our interpreter.

    \clearpage


\section{Technical Solution \& Implementation}
  \blfootnote{Any absolute file paths presented are relative to the root directory of
  the repository for the code.}
  In this section we'll be walking through the code (Python code) that
  implements the entire Lisp language that we've designed. Somewhere around
  the beginning of writing the implementation of the language I realised I would
  need a name for the language, and I settled on \textsc{Lispy}, which is
  essentially just `Lisp' and `\underline{Py}thon' put together. Python files
  do end in the extension \code{.py}, and hence the file extension we'll
  be using for Lispy program files is \code{.lispy}.  It's perhaps not the most
  creative name, and no doubt a name that's (probably) been used before, by someone else
  writing their Lisp in Python.

  I'd like to note that all testing, development and executing of the described
  implementation has been done on a GNU/Linux machine. On such a machine, executing
  either the interpreter or the REPL is as simple as being in the root directory
  of this repository and typing \code{./execute <filename>} or \code{./ilispy},
  respectively.  The same should be true for most UNIX machines, such as a
  BSD distribution or Apple's macOS (provided Python $>$3.7 is installed).

  For Windows I strongly recommend you run the programs in the CMD or PowerShell
  shells, and not the subpar terminal emulator provided with the Python IDLE.
  To run the files in said shells, I believe you may open a CMD window in the
  root of the repository and type \code{python execute <filename>} provided that
  Python is in your PATH environment variable.

  \clearpage

  The file structure of the repository is easy to follow and looks like this:
  \graph{files}{Repo. file structure.}

  \clearpage

  The \code{lispy} folder contains everything from the lexer, to the evaluator,
  with some extra files for tweaking it's behaviour and tackling errors.
  The \code{\_\_init\_\_.py} file ties all the files in the folder together into a
  neatly wrapped Python module, such that externally, in another Python file
  (say the \code{./execute} or the \code{./ilispy} file) you can simply do
  \code{import lispy} and access all the language implementation from there.

  \graph{execute_file}{The \code{./execute} file, is the main file that runs
  Lispy programs.}
  The main file responsible for executing code found in actual \code{.lispy}
  files is the \code{execute} file, and takes in arguments from the command
  line, which needs to be one or more Lispy program files. It does this by
  accessing the ARGV variable, containing the passed arguments, similar to
  how you pass \code{argc} and \code{argv} to the \code{main} function in C.
  e.g.
  \begin{minted}{c}
    int main(int argc, char **argv)
    {
      ...
    }
  \end{minted}
  \graph{ilispy_file}{Allows us to run a REPL, in a way such that it will recover
  from any errors, instead of just exiting.}
  The \code{./ilspy} file just provides us with another way of writing Lispy
  code. Instead of reading files and executing them, this allows us to write
  our code interactively, executing and evaluating the code on the fly, statement
  by statement, always observing the return value for each line you write. This is
  excellent for quickly getting used to the language and debugging code. (This
  file also runs in the command line).

  \graph{repl_lispy}{This is the actual Lispy files that implements the REPL.}
  The behaviour of a REPL and it's implementation was already discussed in \ref{repl}.

  \subsection{Lexerical Analysis \& Tokenisation}
    The core concepts of lexical analysis have been outlined in \ref{lexing},
    so we'll only be discussing it's implementation here.

    \subsubsection{Matching through Regular Expressions}
      The first thing we need to lay out in our lexer, is precisely what kind of
      tokens are allowed to exist (and will subsequently understood by the parser)
      and how we match them, i.e. how we identify them and collect them.

      This can be done through the application of regular expression (again
      discussed in \ref{lexing}). The regular expressions we'll be using are
      outlined at the top of out \code{lexing.py} file:
      \graph{lexer_regex}{The set of regex matchable tokens. (\code{/lispy/lexing.py})}
      Some of these really don't require regular expressions, and are in fact
      not matched using them, but I left them there so in order to outline
      what sort of tokens exist.

      Most of the regular expressions are quite self explanatory, but I'd like
      to through a few of them.  The first thing I want to bring your attention
      to are the ``\code{\char`\\A}'' at the beginning of each regex, these
      simply tell the regex compiler that we'll only like to be matching things
      from the \emph{very beginning} of the string, as opposed to anywhere in
      the string or at the beginning of each line or something else.

      You may notice that both atoms and symbols use the same `identifier' string
      for matching, this makes sense as they are indeed both identifiers, and
      will both need to match against things such as letters, underscores, dashes
      and other special symbols (an extended word type) listed in the string.
      You can also see that numerals are only allowed in identifiers
      if an only if preceded by an extended word type.  Atoms may also have a
      number anywhere in them, given that they start with a colon of course.

      \clearpage

      Numerics are matched with what may be a surprisingly long expression, and
      that is because it allows numerals with different bases: hexadecimal, octal and binary;
      and also allows for exponent notation, e.g.

      \begin{center}
        \begin{tabular}{lll}
            \code{4e10}     & $\iff$  & $4 \times 10^{10}$\\
            \code{5.3e+22}  & $\iff$  & $5.3 \times 10^{22}$\\
            \code{345e-61}  & $\iff$  & $345 \times 10^{-61}$\\
            \code{0b00101}  & $\iff$  & $101_2 \textrm{ or } 5_{10}$\\
            \code{0xff}  & $\iff$  & $\textrm{ff}_{16} \textrm{ or } 255_{10}$\\
            \code{0o771}  & $\iff$  & $771_8 \textrm{ or } 505_{10}$\\
        \end{tabular}
      \end{center}

    \subsubsection{Tokens and Token Streams}
      \graph{token_obj}{The quite simple token object. (in \code{/lispy/lexing.py})}
      The token object takes in: A string identifying what kind of token it is;
      another string of what the actual value of the token is (i.e. what we
      matched); and a hash-map of the location that the token finds itself
      lexically (a line number and column number, the filename, as well as the span (the length
      of the matched string) that's computed by the constructor function itself).
      The \code{\_\_str\_\_} method also provides a string representation of the
      token, which will be useful for debugging.

      Example usage might look something like:
      \begin{minted}{python}
        Token('NUMERIC', "310e-5", {
            'line': 12,
            'column': 4,
            'filename': '~/scripts/some-code.lispy'
        })
      \end{minted}

      \graph{token_stream}{The token stream, essentially a specialised list.
      Some method bodies have been omitted for the sake of brevity.}
      The \code{TokenStream} object allows us to manage a vector of tokens,
      a bit like a Python iterator, by incrementing an internal instance
      variable (\code{self.i}) keeping track of where we are in the stream, and
      is controlled by methods such as \code{TokenStream\#next} and
      \code{TokenStream\#back} and we can get the current token through \code{TokenStream\#current}.


      The Lexer itself is implemented in the \code{lex} function, it takes a
      program string and returns a \code{TokenStream} type:

      \graph{main_lex}{The \code{lex} method, some lexer rules have been omitted
      to fit in the picture.}

      \graph{lex_numeric}{Using the Regular Expressions to match a numeric
      in the Lex function.}

      Above, we can see we assign our match variable, to a potential match
      between a numeric, and the current stage our code is at. If that match
      happened to return a valid match, and not \code{None}, we'll create a new
      numeric token, with type \code{'NUMERIC'} and the matched string, which is
      returned by \code{match.group()}. Location of the token is given by the
      current \code{line} and \code{column} variables, \code{filename} is also
      passed into the dictionary. We need to advance the character pointer
      (\code{i}) by the size of the matched string, and increment the
      \code{column} variable by the same amount too.

      \subsubsection{Parentheses Balancer}
      \graph{paren_bal}{In such a parenthesis heavy language as Lisp, a
      parentheses balancer can be very useful.}

      The paren-balancer essentially counts the amount of open parens, and tells
      you if you have too many or too few parentheses. If you have enough opens
      vs. closing parentheses, but have arranged them in an illogical manner
      (e.g. ``\code{)(}''), it will end up with a negative open vs. close
      count, and tell you, you have an issue with how you've arranged your
      parentheses.

  \subsection{Parser}
    The parser for many Lisps is a relatively simple stage when compared to other
    programming languages.  This fact is due to Lisp's use of parentheses and
    Polish notation, eliminating the need for some of the \emph{harder} parts
    of parsing more natural languages, such as operator precedence and such.

    Everything in Lisp is either atomic, i.e. a datum, or a list, which makes
    constructing the parse tree, even simpler.

    The parser when I was writing the code was only about 42 lines of code
    for a long while, and was entirely functional, complete with error handling
    and everything. Now the parser is ca. 240 lines of code, and that was
    because at some point, I finally added macros to the language, adding a lot
    of search and replace complexity to the parsing stage when expanding macros.

    The main components that constitute the parse-tree data structure and
    the token to parse-tree converter (i.e. `the parser') are defined across two
    files: \code{/lispy/tree.py} and \code{/lispy/parsing.py}.

    \clearpage

    \subsubsection{Abstract Syntax Tree}
      The syntax tree (`Abstract Syntax Tree' $\Rightarrow$ `AST'), is described
      in terms of its components/nodes/branches/leaves in the \code{/lisp/tree.py}
      file, containing a number of classes and abstract classes describing it.
      The classes have variables used to hold the node's value and node-type,
      but also methods such as \code{\_\_str\_\_} which also provide a visual
      representation of the tree and its nodes, which has actually been quite
      helpful through debugging.

      The first class is \code{Tree} which is essentially a wrapper for a generic
      Python list, in fact, the class inherits the list type/class, so it inherits
      all your normal Python list functionality, with the addition of a new
      \code{\_\_str\_\_} method; a variable called \code{self.file} that keeps
      track of which file that AST comes from, and an alias for Python's
      \code{self.append} called \code{self.push}, just so I can keep a consistent
      naming of methods. The code for it looks like this:

      \graph{ast_Tree}{Defined at the top of \code{/lispy/tree.py}}

      Most of the code here is trivial except from perhaps the the string
      representation method, which uses special Unicode characters (denoted by
      the \code{'\char`\\u\#\#\#\#'} strings) to draw special box-drawing
      characters\footnote{\url{https://en.wikipedia.org/wiki/Box-drawing_character}}
      which help draw the branches and trunk of the AST, in character
      represented form.

      \clearpage

      The next three classes / node definitions are three abstract classes,
      not meant to be used directly, but to be inherited from.
      The first one is actually a doubly abstract class, as the next two
      abstract classes inherit from that one.  The first class is just called
      \code{Node}, and it is the mother-class of all subsequent possible
      AST elements, it just describes/outlines the basic behaviour of any node.
      It is described as such:

      \graph{tree_Node}{All nodes have at least: a type; a value; and a name.}

      The \code{TAB = ' ' * 3} at the top just means that a tab will be the
      equivalent of three spaces, when representing the AST/Nodes.

      We can see the node also implements a string representation method, using
      the same box-drawing characters to denote belonging of certain values.

      The \code{self.value} variable will hold the value for the node, say for
      example \code{5} for a Numeric node, or \code{"Hello!"} for a String node.
      \code{self.type} is the classe's own type, which is in this case \code{Node}, and
      that type is Python given, and accessed though the inherent \code{self.\_\_class\_\_}
      variable that exist in every class. The next variable (\code{self.name})
      is related, but provides a string representation of the type (in this
      case \code{'Node'}). This is done by calling \code{str(...)} on the class-type
      variable, but, this will include super-classes, and namespaces, separated
      by dots, so the \code{str} method actually yields something like:
      \code{"<class 'lispy.tree.Node'>"}, so we just split the string by the dots, and
      pick the last element, and the ignore the two last characters
      (the ``\code{'>}''), to give us \code{'Node'}.

      \clearpage

      The \code{Operator} class is the second abstract class and inherits from
      \code{Node}, it is probably the most complicated looking thing in the
      \code{/lispy/tree.py} file, but that's just because of its complicated
      string representation.  The \code{Operator} class describes all nodes in
      the language that can have multiple children (i.e. function calls). The
      \code{Operator} class looks like this:

      \graph{tree_Operator}{Inherits from \code{Node}.}

      \code{Operator} still has the normal \code{Node} variables, but with a few
      extra. First of all, the \code{self.value} now represents the head of the
      list, thats to say, the callee of the function (the operator itself)
      if this were a function call (i.e. it's not unevaluated). The operands
      to the call, (i.e. the tail of the list)  is, well, \code{self.operands}.
      \code{self.shorthand} is set to \code{None}, and basically indicates
      whether this call is a shorthand-lambda, that's to say it's arguments are
      implicit and listed by numerals, more on this concept later. Finally,
      we have a new \code{self.location} variable which just hold information
      on where this node corresponds to in the file (line number, span, column, \etc),
      which will be very helpful if we need to throw errors.

      The \code{\_\_str\_\_} method was put together very
      quickly, as I didn't want to spend too long working on just the small
      step of the parser, that is trying to represent the AST as a string, it
      works by, again, using a number of box-drawing characters and calling
      the string methods on all its children, and creating something resembling
      a tree.  If I had more time to implement the visualisation this is certainly
      code that I would improve/re-write.  It made sense when I wrote it, not
      so much anymore, more than anything its messy.

      Finally, onto our last abstract class, the \code{Data} class:

      \graph{tree_Data}{Abstract node class for all atomic datatypes (any datum)}

      \code{Data} is almost no different from \code{Node}, except now it's also
      got a location variable.

      Let's now look at the child classes that use these two abstact classes:

      \graph{tree_Nodes}{All child nodes all either inherit from \code{Operator}
      or \code{Data}.}[9cm]

      The only class that inherits from \code{Operator} is \code{Call}, this
      could change in future versions of the language (?), but really, this
      is the only one we need in a relatively standard Lisp-style language.
      We've implemented the nodes for the basic LISPY datatypes (\code{Numeric},
      \code{Symbol}, \code{Atom}, \code{String}, and the slightly
      more special, \code{Uneval}, \code{Yield}). However, we're missing nil,
      and since \code{Nil} doesn't really hold any value other than just being
      nil (nil just represents the absence of a value), it is not inherited from
      \code{Data}, nor \code{Operator}. Nil is just defined as:

      \graph{tree_Nil}{\code{Nil} type AST node.}

      That concludes the \code{/lispy/tree.py} file, these components will be
      vitally important in not only parsing, but when visiting the AST for
      evaluation of the tree.

      In the \code{/lispy/tree.py} we will actually be assembling the AST together

    \subsubsection{Bottom-up / Shift-reduce Pattern}
      Our parser will be implemented as a bottom-up parser, generating the
      leaves of the AST and working our way inwards towards the roots. This has
      been discussed more in depth in section \ref{ast}.

      We need to import all our older files, in order to use the functionality
      we've so far implemented, and define some useful variables, like what
      an EOF is and the error handler, which I'll go through later and a simple
      numerical parser, which converts strings to actual Python integers. These
      are all just meta-utilities:

      \graph{parser_header}{Header of the \code{/lispy/parsing.py} file.}

      Let's look at the bottom of the file, where the parser is actually
      defined:

      \graph{parser_parse}{The simple parser loop.}

      The \code{parse} function takes in a stream and
      starts out by defining the \code{EX} variable for
      throwing exceptions in case of bad code, and purges useless tokens from
      the stream. If we have no AST variable defined, \code{parse} will define
      it with an empty tree.

      We define the \code{parse\_loop} function, which steps through the stream,
      using \code{stream.next()} and at each time it encounters a parsable token,
      it calls \code{atom} on it. If it has reached an EOF token, it will know
      its reached the \underline{e}nd \underline{o}f the \underline{f}ile.
      The function calls itself recursively to advance to the next token.
      The function ultimately returns the fully-grown AST.

      The \code{atom} function is where all the shift-reduce functionality of
      the parsing actually lies. No, the name has nothing to do with the datatype
      in our language called atoms, it simply means it parses atomic data, however
      this is now untrue, after I added the \code{Call} type parsing capability
      to it, I just never came up with another name. It is defined as such:

      \graph{parser_atom}{Parses atomic data and function calls, line 190 in
      \code{/lispy/parsing.py}.}

      The \code{atom} function is relatively simple, if it finds a number, it
      will return a Numeric Node, and so on for all the other types, if it finds
      an \code{L\_PAREN} token, it will search for the corresponding \code{R\_PAREN}
      token. While going looking for the \code{R\_PAREN} it will recursively call
      \code{atom} on all the tokens in between, pushing them all to an array.
      When it finds the \code{R\_PAREN}, it will create a \code{Call} node and
      add all its children to it, finally returning the \code{Call} node.


    \subsubsection{Preprocessing \& Macro Expansion Phase}
      This phase was actually added \emph{after} I had finished the interpreter,
      and a lot of the Prelude library.  It is responsible for simply finding
      macro definitions (macro definitions are very similar to C's \code{\#define}
      preprocessor directive) and searches for calls with the same symbol as the
      definition. The code for this is as follows:

      \graph{parser_pre}{The preprocessor function, it tries to expand any
      macros found in each branch of the syntax tree.}

      \graph{parser_macroexpand}{The macro expansion function defines a way of
      searching the tree and replacing any invocation of a macro with a proper
      expansion}
      The method seems pretty heavy, but really all it is, is a search and replace.
      When a macro is being called upon (e.g. \code{(my-macro 3 4)}), we look through
      our macro dictionary (\code{MACROS}) and if we find that it is indeed a macro,
      we replace it with an invocation of that macro (e.g.
      \code{MACRO['my-macro'].invoke(3, 4)}, you can see this happening on line
      134 of \code{parsing.py}).  Of course, when we have a macro called
      \code{my-macro}, that takes two arguments, say \code{x} and \code{y}, we
      need to, again, search and replace all instances of \code{x} and \code{y}
      with their invoker's parameters, (3 and 4 respectively in our last example).

      So far we've been talking about the macros in the \code{MACROS} dictionary
      as some unknown object with methods such as \code{.invoke(...)} implemented
      in them and all sorts. Let's actually have a look at the definition of that
      \code{Macro} object, at the top of our \code{parsing.py} file:

      \graph{parser_macro-obj}{The macro object class and it's methods.}
      The \code{Macro} object has a few important instance variables, and an
      invoke method, which we discussed earlier. How the invoke function works,
      is it takes in the parent call object of the actual invocation, checks through
      the argument list of the call, and will use those arguments to replace their
      corresponding usage in the body of the macro.  First of all we will check
      the length of the argument list of the parent call, and check the length of
      its internal list of arguments, and check if they have the same length.
      If they do have the same length, we create a name map of the arguments, and
      have them map to their corresponding invoker arguments, so that we can then
      go through the macro body and replace those symbols with the correct expressions
      we passed in.

      An example of this could be a macro define as such:

      \begin{minted}{lisp}
        (define macro (sum-and-add-one a b)
          (+ a b 1))

        (print (sum-and-add-one (+ 3 1) 5))
      \end{minted}

      after expansion, this code would be directly equivalent to:

      \begin{minted}{lisp}
        (print (+ (+ 3 1) 5 1))
      \end{minted}

      Hopefully this shows how macros are really just text replacement.


  \subsection{Interpreter/Evaluator}
    Finally onto the interpreter, the thing that will actually evaluate and
    reduce our code into our desired results.

    There are many options I could have gone with here, first of all, you might
    wonder, why an interpreter and not a compiler for example?  Well, the first
    reason for that is that my Lisp dialect is dynamically typed, making it
    hard to compile, as usually compiled languages do all their type checking at
    compile time as opposed to at runtime, with dynamically typed languages.
    Unless we secretly implement static typing through use of
    type inference, but that would still limit our language in other ways.

    The second reason for making it not compiled, is that I would have spend
    a lot of time, reducing the AST into a stream of CPU-architecture specific
    machine-code, which would not only be a large task, but would include me
    having to learn x86-64 assembly language, and even then, it would only
    work on \emph{some} CPUs.

    Hence, I chose to go interpreted, in which case I had (in general) one of
    two options, an AST traversing interpreter or a bytecode assembler with
    a corresponding virtual machine.

    My interpreter ended up being a single pass AST visitor, which executes as it
    walks down the tree. This is slower then compiling for a virtual machine,
    but having to build a virtual machine, a load of instructions and reducing the
    AST to the VM specific bytecode instructions would be a much larger task, and likely
    not something I could complete in time for this NEA.

    \subsubsection{Recursive Decent Evaluation}
      %% Talk about mutually recursive functions.
      The phrase `recursive decent' is usually used to describe a certain type
      of top-down parser, but I feel it also describes, and works in an analogous
      fashion to my interpreter.

      The interpreter is defined as a set of mutually recursive functions,
      all starting from a little loop which calls the evaluate function on
      each branch of the AST, which itself calls a load of other functions which
      help evaluate certain things (mostly these are the internal macros) which
      then \emph{again} call the evaluate function, evaluating other
      sub-expressions. This results in the evaluator evaluating the innermost
      expressions first before it can fully evaluate the parent-expressions.


    \subsubsection{Visiting \& Walking the Tree}
      First let's have a look at the entry-point functions of the interpreter:

      \graph{interp-visit}{The \code{visit} function, goes through each branch and
      evaluates each of them. Defined in \code{/lispy/visitor.py} on line
      \code{1126}.}

      The first thing we see, is that the AST has any potential macros expanded
      before we do anything with it. We make sure all our global variables are
      set up for the execution process, and then we evaluate the branch of the
      AST found at index location `\code{pc}'. The \code{evaluate} call is wrapped
      in \code{try} statement which is set up to catch any \code{RecursionError}
      that may be thrown, due to the maximum recursion depth being exceed (LISPY
      doesn't have any tail-call optimisation and every recursive function call
      in LISPY is equivalent to a recursive call in Python, both of which are
      unfortunate scenarios and something I'd fix in future iterations of the
      language).

      \graph{interp-walk}{The \code{walk} method calls \code{visit}, while
      also doing some extra work, such as loading in the prelude library
      for the first time}

      \clearpage

      The \code{evaluate} method is at the centre of all execution in the
      interpreter.

      \graph{interp-eval}{The \code{evaluate} method takes any AST node
      / subtree and evaluates it, or, in the case of a fully evaluated
      atomic datum being passed in, it will simply return it back again.}

      The method simply takes in a node, and runs through a plethora of
      if-statements and returns an evaluation depending on what type of data
      you passed in, often calling itself recursively.

      When evaluating a symbol, it first looks through the dictionary of known
      internal macros, if it isn't an internal macro, it will look through the
      known symbol tables of the current scopes and return the value bound to it
      in the table it was found in. A similar thing happens when it tries to
      evaluate an atom, if the atom already exists, it'll return the uniquely
      hashed \code{Atomise} object, otherwise, it'll add a new one.

      If the evaluator encounters a \code{Call} type object, it will call the
      `\code{execute\_method}' method:

      \graph{interp-exec}{\code{execute\_method} is responsible for managing
      the symbol tables of a method call and the call stack,
      before and after evaluation.}

      This method has many edge cases to consider, like the method called being an
      internal macro for example, but ultimately it is meant to manage the calls
      symbol tables before and after calling.  First it evaluates the caller, if the
      caller evaluated to a \code{Definition} object, that means it is a valid
      function call (a call to, say, the number three \code{(3)}, would throw an error,
      as we cannot call a number).  The \code{Definition} object has the methods
      corresponding symbol table, which will be pushed onto the call stack, so that
      any local variables and arguments bound inside the function scope can be accessed.
      Once this is done we call the \code{Definition} object's \code{call} method, which
      clones the function body and calls, once again, the \code{evaluate} method on it.
      Once this is finished, we pop off the call stack, and any frozen super-scope tables
      we might have added, and return the result of the evaluation, i.e. the result of
      \code{definition.call()}.

      \clearpage

      So far we've mentioned a number of objects and classes needed for our above, code.
      Namely, the \code{SymbolTable}, \code{Definition} and \code{Atomise} class.

      \graph{interp-Def}{The \code{Definition} class.}

       The class responsible for packaging method definitions must hold:
       \begin{itemize}
         \item The method's parameter list.
         \item The method body.
         \item A refrence to the symbol table
       \end{itemize}

       The class also has a method, which clones the method body and evaluates it.

       \clearpage

       \graph{interp-Atom}{\code{Atomise} class.}

       The \code{Atomise} class has little functionality, it is simply a wrapper.
       It ensures that all atoms in LISPY have the same memory reference, and importantly
       the same hash.

       \clearpage

       \graph{interp-SymTab}{The \code{SymbolTable} class, holds any local variables
       existing within a specific scope.}

       This class is one of the more complex ones, yet quite straight forward in
       its functionality.  It holds a \code{scope} instance variable, which is an integer,
       which is a way to facilitate looking up tables, and they are linked to the hash
       of the parent node the scope encompasses in the AST. It has a \code{name}, linked
       to the name of the function the scope is for; a \code{local} variable, which
       is a dictionary of local variables; \code{mutables} is a list of variable names
       which are allowed to be mutated; \code{args} is a list of which locals are
       exist as function arguments; \code{frozen} is a Boolean, signifying whether the
       table is allowed to be altered in any way.

       Description of notable methods:
       \begin{itemize}
         \item \code{bind} --- Sets a variable in the table, takes name and a corresponding
         value.
         \item \code{declare} --- Tells the table that a certain variable exists, and
         will soon be bound.
         \item \code{declare\_args} --- Informs the table of what arguments its corresponding
         function will have.
         \item \code{give\_args} --- Gives the table the function arguments upon an invocation.
         Here it will also check for the correct number of arguments.
         \item \code{clean} --- Will empty the table of variables, and get it restore it to
         how it was formaly, ready for second use.
       \end{itemize}


       \clearpage



    \subsubsection{Loading External Files / Require Statement}
      LISPY has a \code{(require :folder/name-of-file)} statement for importing, parsing,
      and executing files other than the current file being interpreted.

      This requiring of other files is actually done secretly every time you run the interpreter,
      by requiring the prelude library, for all the basic language functionality.
      You can imagine this as a secret \code{(require :\$LISPY\_INSTALL/prelude/prelude)}
      happening at the top of the file your executing. This is similar to the \code{std}
      (standard) library in C or C++.

      This require statement is implemented as:
      \graph{interp-load_file}{Loads external \code{.lispy} files.}

      which is then used by the \code{require} (\code{\_require\_macro}) internal macro:

      \graph{interp-req}{Parses file-paths and calls \code{load\_file} on them.}

      \clearpage



  \subsection{All Internal Macros}
    An internal macro is just like any LISPY macro, except it's written in and
    interfaces with the implementation language (Python).

    All the internal macros are listed in a global dictionary in the \code{visitor.py}
    file, where each key is the macro name and the corresponding value is a
    reference to the Python function that implements the macro.

    The list of macros is found at line \code{968} and is, as of current, the
    following:

    \begin{minted}{python}
    MACROS = {
        'do': _do_macro,
        'prog': _do_macro,
        'yield': _yield_macro,
        'require': _require_macro,
        'eval': _eval_macro,
        'scope': _scope_macro, # < Mostly for debugging.
        'type': _type_macro,
        'name': _name_macro,
        'if': _if_macro,
        'unless': _unless_macro,
        'list': _list_macro,
        'size': _size_macro,
        'index': _index_macro,
        'iterate': _iterate_macro,
        'push': _push_macro,
        'unshift': _unshift_macro,
        'concat': _concat_macro,
        'concat!': _concat_des_macro,
        'pop': _pop_macro,
        'shift': _shift_macro,
        '<>': _composition_macro, # < Neat, isn't it?
        '+': _add_macro,
        '-': _sub_macro,
        '*': _mul_macro,
        '/': _div_macro,
        '%': _mod_macro,
        '=': _eq_macro,
        '/=':_nq_macro,
        '!': _ne_macro,
        '&&': _and_macro,
        '||': _or_macro,
        '^^': _xor_macro, # < Not a common one.
        '<': _lt_macro,
        '>': _gt_macro,
        '<=':_le_macro,
        '>=':_ge_macro,
        'string': _string_macro,
        'repr': _repr_macro,
        'ast': _ast_macro, # < Mostly for debugging.
        'out': _out_macro,
        'read': _read_macro,
        'puts': _puts_macro,
        'let': _let_macro,
        'delete': _delete_macro,
        'mutate': lambda node: _let_macro(node, mutable=True),
        'λ': _lambda_macro,
        '->': _shorthand_macro,
        'define': _define_macro,
    }
    \end{minted}

    \clearpage

    The implementation of each internal macro will take a long time to go over,
    so I'll leave their implementations and their helper functions out of this
    document, so if you need to refer to their implementation, see
    \code{/lispy/visitor.py} on line \code{343} to line \code{966}.

    An outline of what each  internal macro does will be outlined in the
    following subsections of this subsection:

    \subsubsection{\code{do} \& \code{prog}}
      Takes a variadic amount of arguments, each argument is executed
      subsequently, essentially allowing you to just chain multiple
      statements together as a block.

    \subsubsection{\code{yield}}
      Allows you to \emph{return} or \emph{yield} from a function.
      This is only useful if you're inside a \code{do} block and want
      to return early.

    \subsubsection{\code{require}}
      Takes in any number of \emph{name nodes}, which is either:
      \begin{itemize}
        \item An Atom
        \item A string
        \item An unevaluated symbol.
      \end{itemize}

      The name node will we read as a file path, and loaded in to the program.

    \subsubsection{\code{eval}}
      Takes a single argument.  It will evaluate anything that was
      previously unevaluated. If already evaluated, it will do nothing and
      return its argument.

    \subsubsection{\code{scope}}
      Takes exactly one symbol, will return a list containing the name of the
      scope, and the scope hash for the scope that the given symbol finds
      itself in.

    \subsubsection{\code{type}}
      Takes in one argument of any type, and returns an atom with the name
      of the type of the evaluated argument that was passed in.

    \subsubsection{\code{name}}
      Converts any name node into an atom with the same \emph{name}.

    \subsubsection{\code{if}}
      An if statement takes 2 or 3 arguments. The first argument is the
      condition, the second the consequence, and the third (if given)
      is the alternative (if-this-then-that-otherwise-that).
      The first argument must evaluate to something true or false,
      if the first argument is true the second argument gets evaluated,
      if not, the third argument gets evaluated (if it exists).

    \subsubsection{\code{unless}}
      The very same as the if statement, except it negates the condition,
      and proceeds as normal.

      \begin{minted}{lisp}
        ;; The following statements are equivalent:

        (if (/= a b)
          (puts "a and b are different")
          (puts "a and b are the same"))

        (unless (= a b)
          (puts "a and b are different")
          (puts "a and b are the same"))
      \end{minted}

    \subsubsection{\code{list}}
      Creates a list of all it's arguments, this is similar to the
      quote, but here all the arguments get evaluated.

    \subsubsection{\code{size}}
      Takes in a single list or string, returns the length of the
      elements.

    \subsubsection{\code{index}}
      Takes exactly two arguments, the first argument is a signed
      integer that represents an index number of the array starting from
      zero, negative indices represent accessing elements from the end of
      the list.

    \subsubsection{\code{iterate}}
      Takes a single statement and executes it over and over again, forever.
      The loop can only be broken if a the statement evaluates to a \code{break}.

    \subsubsection{\code{push}}
      Takes two arguments, the first one can be anything, and the second
      one needs to be a list.  The fist argument will be appended to the
      the end of the list.

    \subsubsection{\code{unshift}}
      Same as push, instead it prepends to the list as oppsed to
      appending.


    \subsubsection{\code{concat}}
      Takes any number of lists or strings and merges them together as
      a new list or string.

    \subsubsection{\code{concat!}}
      The exclamation mark at the end signifies that the method is
      destructive. This works the same as \code{concat} but the first
      argument must be a symbol, and that symbol will be mutated to
      the result of the concatenation.

    \subsubsection{\code{pop}}
      Removes the last element from the list that is passed in.

    \subsubsection{\code{shift}}
      Removes the first element from the list that is passed in.

    \subsubsection{\code{<>}}
      Takes in any number of arguments, all of which \emph{must} evaluate
      to \emph{functions}, these functions will be composed together.

      For example:
      \begin{minted}{lisp}
        (f (g (h (i (j (k x y)))))) ;; read: f of g of h of i of j of k of x and y.
        ;; This can be better written as a composition.
        ((<> f g h i j k) x y) ;; Far fewer parentheses, easier to read, and shorter.
      \end{minted}

    \subsubsection{\code{+}}
      Takes any number of all numbers or all strings and adds
      them all together.

    \subsubsection{\code{-}}
      Takes any number of numbers and finds their difference.

    \subsubsection{\code{*}}
      Takes any number of numbers and finds their product.

    \subsubsection{\code{/}}
      Takes any number of numbers and finds their quotient.
      Will always return a float no matter what type the arguments are,
      unless the numbers divide each other evenly.

    \subsubsection{\code{\%}}
      Takes any number of numbers and returns their modulo.

    \subsubsection{\code{=}}
      Will check if all its arguments are equal to each other.

    \subsubsection{\code{/=}}
      Will check if at least one of its arguments are not equal
      to the rest.

    \subsubsection{\code{!}}
      Negates the \emph{one} argument given.

    \subsubsection{\code{\&\&}}
       Checks if all arguments evaluated to something \emph{truthy}.

    \subsubsection{\code{||}}
      Checks if at least one argument or more are truthy.

    \subsubsection{\code{\^{}\^{}}}
      Checks if an even number of arguments are truthy (XOR --- Exclusive Or).

    \subsubsection{\code{<}}
      Checks if each subsequent argument is less than the last.

    \subsubsection{\code{>}}
      Checks if each subsequent argument is greater than the last.

    \subsubsection{\code{<=}}
      Checks if each subsequent argument is less than or equal the last.

    \subsubsection{\code{>}}
      Checks if each subsequent argument is greater than or equal the last.

    \subsubsection{\code{string}}
      Tries it best to convert its argument into a string
      representation.

    \subsubsection{\code{repr}}
      Converts a string to its unescaped version.

      e.g. It would return the string backslash `n' (``\char`\\n'') when
      given a string with a new-line in it.

    \subsubsection{\code{ast}}
      Returns a string representation of the unevaluated syntax tree
      of the argument you passed in.

    \subsubsection{\code{out}}
      Will print the string representation of any number of arguments
      passed into it, with no seperation between the strings and no
      trailing new line appended implicitly to STDOUT.

    \subsubsection{\code{read}}
      Will wait for user input from STDIN, and takes it when the user gives a
      new line.
    \subsubsection{\code{puts}}
      Does the same as \code{out}, but each string is separated by a space,
      and adds a trailing new line.

    \subsubsection{\code{let}}
      Takes in any number of symbol-value pairs (e.g. \code{(sym val)};
      \code{(x 3)}) where the first value is a symbol which the value will
      get bound to.

      For example:
      \begin{minted}{lisp}
        (let (x 3) (y 7))  ;; Read as: `let x = 3 and y = 7.'
      \end{minted}

    \subsubsection{\code{delete}}
      Takes any number of symbols and deletes them from their
      symbol table thereby removing their bindings.

    \subsubsection{\code{mutate}}
      Same as let, except it allows the mutation of its symbols.

    \subsubsection{\code{λ} \& \code{lambda}}
      Takes two arguments. The first one is a list of symbols
      and the second can be any expression. The first list represents
      the arguments being given to the anonymous function / closure / lambda
      and the second argument is the lambda body, which may use the arguments
      given to it listed in the argument list.

      \code{λ} has been aliased to \code{lambda} in the prelude
      library.

    \subsubsection{\code{->}}
      This is a shorthand lambda. It works the same as normal lambda but
      it doesn't need an argument list. This is because the number of
      arguments are deduced at the parse stage, this is possible as
      arguments are simply listed numerically with a percentage sign
      in front.

      For example:

      \begin{minted}{lisp}
        (-> (+ 5 %1 %2))  ;; A function that takes two arguments and
                          ;; returns their sum plus five.
        ;; This is equivalent to:

        (lambda (n m) (+ 5 n m))
      \end{minted}

    \subsubsection{\code{define}}
    This is similar to the lambda, except it gives the function a name.
    The first argument must be a list, the list must contain first
    the function name, and the rest are the function parameters.
    The second argument is the function body.

    For example:
      \begin{minted}{lisp}
        (define (f x y)
          (+ 5 x y))
        ;; Which is equivalent to
        (let (f (lambda (x y) (+ 5 x y))))
      \end{minted}

    \clearpage

  \subsection{Debugging, Configuration \& Verbose Mode}
    In the file \code{/lispy/config.py} it is possible to alter
    the behaviour of the execution slightly.

    The first number of variables listed are recursion depth limits,
    which allow Python's call stack to grow to a certain size by
    the line of code:

    \begin{minted}{python}
    RECURSION_LIMIT = YOUR_LIMIT_HERE
    sys.setrecursionlimit(RECURSION_LIMIT)
    \end{minted}

    Right now the recursion limit is very high, which \emph{could} lead
    to some overflows.

    The following constants: \code{DEBUG, EXIT\_ON\_ERROR, RECOVERING\_FROM\_ERROR}
    change slightly the behaviour / output of the program.

    \code{DEBUG} --- Will make the interpreter output information about
    just about anything it is doing. You may have seen many lines of code
    saying something similar to: \code{if conf.DEBUG: print(...)},
    this is what they're for. The debug mode has been very useful while
    developing the components of the interpreter.

    \code{EXIT\_ON\_ERROR} --- Is another Boolean, it dictates whether the
    program should continue executing after encountering an error.
    This is useful of REPLs as the REPL should still continue working
    even if  you write a faulty line of code.

    \code{RECOVERING\_FROM\_ERROR} --- Is just a global that keeps
    track of whether or not the program has recovered form an error yet.

  \subsection{Prelude / Standard Library for the Language}
    The Prelude is simply a set of files that run before \emph{your} program
    begins to execute. The library outlines some simple functions, globals, and
    macros which are useful for writing your code. Examples of things the prelude
    implements are: The \code{while} statement; the \code{for} loop; functions
    from functional programming such as \code{map}, \code{reduce} and \code{filter};
    \etc

    The current files in the prelude are (in load order):
    \begin{itemize}
      \item \code{functional.lispy} --- Basic functional functions, this file implements:
        \begin{itemize}
          \item \code{lambda}, which is aliased to \code{λ}.
          \item \code{apply} which is a macro that takes a function
                and applies it to a list of arguments as that functions
                argument list.
        \end{itemize}
      \item \code{destructive.lispy} --- All destructive operations, this file implements:
        \begin{itemize}
          \item \code{incr!}, which takes a symbol and a number, and increments
                the symbol by that number.
          \item \code{decr!}, which does the same as \code{incr!} except it
                decrements instead.

          \item \code{+1}, which increments a symbol by exactly one.
          \item \code{-1}, which decrements a symbol by exactly one.
        \end{itemize}
      \item \code{dt.lispy} --- Defines a number of functions for workin with
      datatypes, this file implements:
        \begin{itemize}
          \item \code{to\_string}, which converts its argument to a string.
          \item \code{to\_atom}, which converts its argument to an atom.
          \item \code{type?}, which checks if its first arguments type matches
          the type of the second argument.
          \item \code{nil?}, which checks if its argument is of type NIL.
          \item \code{atom?}, which checks if its argument is of type ATOM.
        \end{itemize}
      \item \code{loop.lispy} --- Defines a number of different styles of loop,
      this file implements:
        \begin{itemize}
          \item \code{while}, which will repeat its second argument, as long
          as its first argument keeps being true.
          \item \code{until}, which will repeat its second argument, as long
          as its first argument keeps being false.
          \item \code{loop}, which is a a loop that will run forever.
          \item \code{from}, which will run a function a given amount of times,
          from a given range, where the argument supplied to that function is
          the current position through that range.
          \item \code{times}, will run a function a given amount of times.
        \end{itemize}
      \item \code{lists.lispy} --- A collections of functions for list operations,
      this file implements:
        \begin{itemize}
          \item \code{quote}, which simply quotes an expression, just like the
          single quote operator, however here it's named.
          \item \code{push, unshift, concat, concat!} have all been aliased to:
                \code{append, prepend, merge, merge!}, respectively.
          \item \code{empty?}, which takes a list and returns a Boolean depending
                on whether the list is empty.
          \item \code{first}, which returns the first item of a list.
          \item \code{last}, which returns the last item of a list.
          \item \code{head} is an alias of \code{first}.
          \item \code{tail}, returns a list of all elements of the given
          list excluding the first element of the list.
          \item \code{for}, which implements a for loop with the syntax:
                \code{(for some-symbol in some-list (do execute this body))}.
          \item \code{reverse}, which reverses a given list.
          \item \code{fill}, which fills a given array with integers between a
          specified range.
          \item \code{range}, which returns an array filled with integers from
          a given lowe bound and upper bound.
          \item \code{map}, which takes a function and a list and applies
          that function to every element of that list.
          \item \code{reduce}, which will apply an operation (given by a function)
          between every element of the given list, reducing it down to one value.
          \item \code{filter}, which takes a function which is applied to every
          element of a supplied list, and if that function evaluates to true, it
          is appended to a new list, which is then ultimately returned.
        \end{itemize}
      \item \code{numerics.lispy} --- Defines a set of functions for used for
            doing work on numbers, this file implements:
              \begin{itemize}
                \item \code{zero?}, which takes a number and returns whether the
                number is zero or not.
                \item \code{divisible?}, checks whether your first argument is
                divisible by the second argument.
                \item \code{sum}, which returns the sum of a list. It is
                defined as \code{(define (sum l) (apply + l))}.
              \end{itemize}
      \item \code{IO.lispy} --- Functions relating to input and output, this file implements:
        \begin{itemize}
          \item \code{EOF}, which is a variable that has value of: \code{"\char`\\0"}.
          \item \code{out, read} have been aliased to \code{print, input}
          respectively.
        \end{itemize}
    \end{itemize}

    The file \code{/prelude/prelude.lispy} requires all the files above, and looks
    like this.

    \graph{prelude}{The prelude file that gets loaded at the beginning of every
    interpreter spawn.}

    It is important to note that the Prelude is entirely implemented in LISPY
    itself, this demonstrates that the language is not only quite usable, but also
    extendable in its syntax, mostly through the use of macros.

  \clearpage

\section{Testing of Implementation}
  I have been continuously the implementation of the language as I've been
  developing it, many of the sample code, and all of the debug/verbose prints
  found throughout the code were done long before the language implementation
  was finished.


  \subsection{Testing Discrete Sub-Components}
    Each stage of the interpreter has many small print statements that will
    print if the \code{DEBUG} constant is set to \code{True} in the \code{config.py}
    file, these allow me to see what every stage of the interpreter is doing.

    It will be impossible to replicate and document the hundreds of tests
    I've run throughout the development of the interpreter, but I will run through
    an example of how I tested these sub-components.

    Most of my testing has been done in a file found in the root directory of
    the repository called `\code{/testing.lispy}', which currently happens
    to have something appropriate to use as an example here.

    \code{testing.lispy} looks like this:

    \begin{minted}[linenos]{lisp}
    (define macro (ten-times sym body) (do
      (let (sym 0))
      (iterate (do
        (mutate (sym (+ (eval sym) 1)))
        (if (> (eval sym) 10) break)
        (eval body)))
      (delete sym)))


    (ten-times counter (do
      (puts counter)))
                         ;;       vvv  --- UNDERSCORE
    (-> (+ %1 %2 %3 %4)) ;; Here, `_' just means the return value of the
    (_ 5 5 5 5)          ;;   last statement that has been evaluated.
    (puts (string "5*4 =" _))
    \end{minted}


    Let's now also write a little Python script which will print
    out the exact information we need. It is called \code{/debug-stages.py}
    and looks like this:

    \graph{debug-stages}{Our debugging script.}


    \subsubsection{Lexical Analysis}
      The lexer has been described in detail previously, and produces the
      expected token stream. This is what the above program's token
      stream looks like:

      \begin{Verbatim}[baselinestretch=0.75]
<Token(L_PAREN) ........... [1, 1] ... '('>
<Token(SYMBOL) ............ [1, 2] ... 'define'>
<Token(SYMBOL) ............ [1, 9] ... 'macro'>
<Token(L_PAREN) .......... [1, 15] ... '('>
<Token(SYMBOL) ........... [1, 16] ... 'ten-times'>
<Token(SYMBOL) ........... [1, 26] ... 'sym'>
<Token(SYMBOL) ........... [1, 30] ... 'body'>
<Token(R_PAREN) .......... [1, 34] ... ')'>
<Token(L_PAREN) .......... [1, 36] ... '('>
<Token(SYMBOL) ........... [1, 37] ... 'do'>
<Token(TERMINATOR) ....... [1, 39] ... '\n'>
<Token(L_PAREN) ........... [2, 3] ... '('>
<Token(SYMBOL) ............ [2, 4] ... 'let'>
<Token(L_PAREN) ........... [2, 8] ... '('>
<Token(SYMBOL) ............ [2, 9] ... 'sym'>
<Token(NUMERIC) .......... [2, 13] ... '0'>
<Token(R_PAREN) .......... [2, 14] ... ')'>
<Token(R_PAREN) .......... [2, 15] ... ')'>
<Token(TERMINATOR) ....... [2, 16] ... '\n'>
<Token(L_PAREN) ........... [3, 3] ... '('>
<Token(SYMBOL) ............ [3, 4] ... 'iterate'>
<Token(L_PAREN) .......... [3, 12] ... '('>
<Token(SYMBOL) ........... [3, 13] ... 'do'>
<Token(TERMINATOR) ....... [3, 15] ... '\n'>
<Token(L_PAREN) ........... [4, 5] ... '('>
<Token(SYMBOL) ............ [4, 6] ... 'mutate'>
<Token(L_PAREN) .......... [4, 13] ... '('>
<Token(SYMBOL) ........... [4, 14] ... 'sym'>
<Token(L_PAREN) .......... [4, 18] ... '('>
<Token(SYMBOL) ........... [4, 19] ... '+'>
<Token(L_PAREN) .......... [4, 21] ... '('>
<Token(SYMBOL) ........... [4, 22] ... 'eval'>
<Token(SYMBOL) ........... [4, 27] ... 'sym'>
<Token(R_PAREN) .......... [4, 30] ... ')'>
<Token(NUMERIC) .......... [4, 32] ... '1'>
<Token(R_PAREN) .......... [4, 33] ... ')'>
<Token(R_PAREN) .......... [4, 34] ... ')'>
<Token(R_PAREN) .......... [4, 35] ... ')'>
<Token(TERMINATOR) ....... [4, 36] ... '\n'>
<Token(L_PAREN) ........... [5, 5] ... '('>
<Token(SYMBOL) ............ [5, 6] ... 'if'>
<Token(L_PAREN) ........... [5, 9] ... '('>
<Token(SYMBOL) ........... [5, 10] ... '>'>
<Token(L_PAREN) .......... [5, 12] ... '('>
<Token(SYMBOL) ........... [5, 13] ... 'eval'>
<Token(SYMBOL) ........... [5, 18] ... 'sym'>
<Token(R_PAREN) .......... [5, 21] ... ')'>
<Token(NUMERIC) .......... [5, 23] ... '10'>
<Token(R_PAREN) .......... [5, 25] ... ')'>
<Token(SYMBOL) ........... [5, 27] ... 'break'>
<Token(R_PAREN) .......... [5, 32] ... ')'>
<Token(TERMINATOR) ....... [5, 33] ... '\n'>
<Token(L_PAREN) ........... [6, 5] ... '('>
<Token(SYMBOL) ............ [6, 6] ... 'eval'>
<Token(SYMBOL) ........... [6, 11] ... 'body'>
<Token(R_PAREN) .......... [6, 15] ... ')'>
<Token(R_PAREN) .......... [6, 16] ... ')'>
<Token(R_PAREN) .......... [6, 17] ... ')'>
<Token(TERMINATOR) ....... [6, 18] ... '\n'>
<Token(L_PAREN) ........... [7, 3] ... '('>
<Token(SYMBOL) ............ [7, 4] ... 'delete'>
<Token(SYMBOL) ........... [7, 11] ... 'sym'>
<Token(R_PAREN) .......... [7, 14] ... ')'>
<Token(R_PAREN) .......... [7, 15] ... ')'>
<Token(R_PAREN) .......... [7, 16] ... ')'>
<Token(TERMINATOR) ....... [7, 17] ... '\n'>
<Token(TERMINATOR) ........ [8, 1] ... '\n'>
<Token(TERMINATOR) ........ [9, 1] ... '\n'>
<Token(L_PAREN) .......... [10, 1] ... '('>
<Token(SYMBOL) ........... [10, 2] ... 'ten-times'>
<Token(SYMBOL) .......... [10, 12] ... 'counter'>
<Token(L_PAREN) ......... [10, 20] ... '('>
<Token(SYMBOL) .......... [10, 21] ... 'do'>
<Token(TERMINATOR) ...... [10, 23] ... '\n'>
<Token(L_PAREN) .......... [11, 3] ... '('>
<Token(SYMBOL) ........... [11, 4] ... 'puts'>
<Token(SYMBOL) ........... [11, 9] ... 'counter'>
<Token(R_PAREN) ......... [11, 16] ... ')'>
<Token(R_PAREN) ......... [11, 17] ... ')'>
<Token(R_PAREN) ......... [11, 18] ... ')'>
<Token(TERMINATOR) ...... [11, 19] ... '\n'>
<Token(TERMINATOR) ...... [12, 50] ... '\n'>
<Token(L_PAREN) .......... [13, 1] ... '('>
<Token(SYMBOL) ........... [13, 2] ... '->'>
<Token(L_PAREN) .......... [13, 5] ... '('>
<Token(SYMBOL) ........... [13, 6] ... '+'>
<Token(SYMBOL) ........... [13, 8] ... '%1'>
<Token(SYMBOL) .......... [13, 11] ... '%2'>
<Token(SYMBOL) .......... [13, 14] ... '%3'>
<Token(SYMBOL) .......... [13, 17] ... '%4'>
<Token(R_PAREN) ......... [13, 19] ... ')'>
<Token(R_PAREN) ......... [13, 20] ... ')'>
<Token(TERMINATOR) ...... [13, 69] ... '\n'>
<Token(L_PAREN) .......... [14, 1] ... '('>
<Token(SYMBOL) ........... [14, 2] ... '_'>
<Token(NUMERIC) .......... [14, 4] ... '5'>
<Token(NUMERIC) .......... [14, 6] ... '5'>
<Token(NUMERIC) .......... [14, 8] ... '5'>
<Token(NUMERIC) ......... [14, 10] ... '5'>
<Token(R_PAREN) ......... [14, 11] ... ')'>
<Token(TERMINATOR) ...... [14, 66] ... '\n'>
<Token(L_PAREN) .......... [15, 1] ... '('>
<Token(SYMBOL) ........... [15, 2] ... 'puts'>
<Token(L_PAREN) .......... [15, 7] ... '('>
<Token(SYMBOL) ........... [15, 8] ... 'string'>
<Token(STRING) .......... [15, 15] ... '5*4 ='>
<Token(SYMBOL) .......... [15, 23] ... '_'>
<Token(R_PAREN) ......... [15, 24] ... ')'>
<Token(R_PAREN) ......... [15, 25] ... ')'>
<Token(TERMINATOR) ...... [15, 26] ... '\n'>
<Token(EOF) .............. [16, 1] ... '\x00'>
      \end{Verbatim}

      Here we can see every token labelled with all the correct annotation,
      and accurate line and column numbers. The stream also  provides readable
      strings which are snippets of the code they correspond to. Every token
      stream ends in an EOF token.

    \subsubsection{Initial Parse Tree}

    The script then prints out the initial parse tree, which if you read carefully,
    you can quite clearly see how it corresponds to the original code. Indentation
    in the source file corresponds vaguely to indentation in the tree.

    The tree looks like this:

    \setlength{\lineskip}{0pt}
    \begin{Verbatim}[baselinestretch=0.75]
│
├─<AST::Node[Call]
│      ├─caller
│      │  └─<AST::Node[Symbol] ('define')>
│      └─operands=[
│         <AST::Node[Symbol] ('macro')>
│         <AST::Node[Call]
│            ├─caller
│            │  └─<AST::Node[Symbol] ('ten-times')>
│            └─operands=[
│               <AST::Node[Symbol] ('sym')>
│               <AST::Node[Symbol] ('body')>
│              ]>
│         <AST::Node[Call]
│            ├─caller
│            │  └─<AST::Node[Symbol] ('do')>
│            └─operands=[
│               <AST::Node[Call]
│               ├─caller
│               │  └─<AST::Node[Symbol] ('let')>
│               └─operands=[
│                  <AST::Node[Call]
│                  ├─caller
│                  │  └─<AST::Node[Symbol] ('sym')>
│                  └─operands=[
│                     <AST::Node[Numeric] (0)>
│                    ]>
│                 ]>
│               <AST::Node[Call]
│               ├─caller
│               │  └─<AST::Node[Symbol] ('iterate')>
│               └─operands=[
│                  <AST::Node[Call]
│                  ├─caller
│                  │  └─<AST::Node[Symbol] ('do')>
│                  └─operands=[
│                     <AST::Node[Call]
│                     ├─caller
│                     │  └─<AST::Node[Symbol] ('mutate')>
│                     └─operands=[
│                        <AST::Node[Call]
│                        ├─caller
│                        │  └─<AST::Node[Symbol] ('sym')>
│                        └─operands=[
│                           <AST::Node[Call]
│                           ├─caller
│                           │  └─<AST::Node[Symbol] ('+')>
│                           └─operands=[
│                              <AST::Node[Call]
│                              ├─caller
│                              │  └─<AST::Node[Symbol] ('eval')>
│                              └─operands=[
│                                 <AST::Node[Symbol] ('sym')>
│                                ]>
│                              <AST::Node[Numeric] (1)>
│                             ]>
│                          ]>
│                       ]>
│                     <AST::Node[Call]
│                     ├─caller
│                     │  └─<AST::Node[Symbol] ('if')>
│                     └─operands=[
│                        <AST::Node[Call]
│                        ├─caller
│                        │  └─<AST::Node[Symbol] ('>')>
│                        └─operands=[
│                           <AST::Node[Call]
│                           ├─caller
│                           │  └─<AST::Node[Symbol] ('eval')>
│                           └─operands=[
│                              <AST::Node[Symbol] ('sym')>
│                             ]>
│                           <AST::Node[Numeric] (10)>
│                          ]>
│                        <AST::Node[Symbol] ('break')>
│                       ]>
│                     <AST::Node[Call]
│                     ├─caller
│                     │  └─<AST::Node[Symbol] ('eval')>
│                     └─operands=[
│                        <AST::Node[Symbol] ('body')>
│                       ]>
│                    ]>
│                 ]>
│               <AST::Node[Call]
│               ├─caller
│               │  └─<AST::Node[Symbol] ('delete')>
│               └─operands=[
│                  <AST::Node[Symbol] ('sym')>
│                 ]>
│              ]>
│        ]>
│
├─<AST::Node[Call]
│      ├─caller
│      │  └─<AST::Node[Symbol] ('ten-times')>
│      └─operands=[
│         <AST::Node[Symbol] ('counter')>
│         <AST::Node[Call]
│            ├─caller
│            │  └─<AST::Node[Symbol] ('do')>
│            └─operands=[
│               <AST::Node[Call]
│               ├─caller
│               │  └─<AST::Node[Symbol] ('puts')>
│               └─operands=[
│                  <AST::Node[Symbol] ('counter')>
│                 ]>
│              ]>
│        ]>
│
├─<AST::Node[Call]
│      ├─caller
│      │  └─<AST::Node[Symbol] ('->')>
│      └─operands=[
│         <AST::Node[Call]
│            ├─caller
│            │  └─<AST::Node[Symbol] ('+')>
│            └─operands=[
│               <AST::Node[Symbol] ('%1')>
│               <AST::Node[Symbol] ('%2')>
│               <AST::Node[Symbol] ('%3')>
│               <AST::Node[Symbol] ('%4')>
│              ]>
│        ]>
│
├─<AST::Node[Call]
│      ├─caller
│      │  └─<AST::Node[Symbol] ('_')>
│      └─operands=[
│         <AST::Node[Numeric] (5)>
│         <AST::Node[Numeric] (5)>
│         <AST::Node[Numeric] (5)>
│         <AST::Node[Numeric] (5)>
│        ]>
│
├─<AST::Node[Call]
│      ├─caller
│      │  └─<AST::Node[Symbol] ('puts')>
│      └─operands=[
│         <AST::Node[Call]
│            ├─caller
│            │  └─<AST::Node[Symbol] ('string')>
│            └─operands=[
│               <AST::Node[String] ('5*4 =')>
│               <AST::Node[Symbol] ('_')>
│              ]>
│        ]>

\end{Verbatim}

    \clearpage

    \subsubsection{Macro Expanded Tree}
      Every line beginning with `\code{(define macro (...}' needs to defines a
      macro, and macros are only defined under the context of the preprocessor
      part of the parser, and the evaluator would have no idea what to do with it.
      Hence every occurrence of a macro definition is simply replaced with a nil
      node, as you'll see in the macro expanded tree, as well as the expansion of
      the above macro.

      \clearpage

      The final, macro expanded, abstract syntax tree looks like this:

      \begin{Verbatim}[baselinestretch=0.75]
│
├─<AST::Node[Nil] ('nil')>
│
├─<AST::Node[Call]
│      ├─caller
│      │  └─<AST::Node[Symbol] ('do')>
│      └─operands=[
│         <AST::Node[Call]
│            ├─caller
│            │  └─<AST::Node[Symbol] ('let')>
│            └─operands=[
│               <AST::Node[Call]
│               ├─caller
│               │  └─<AST::Node[Uneval] (<AST::Node[Symbol] ('counter')>)>
│               └─operands=[
│                  <AST::Node[Numeric] (0)>
│                 ]>
│              ]>
│         <AST::Node[Call]
│            ├─caller
│            │  └─<AST::Node[Symbol] ('iterate')>
│            └─operands=[
│               <AST::Node[Call]
│               ├─caller
│               │  └─<AST::Node[Symbol] ('do')>
│               └─operands=[
│                  <AST::Node[Call]
│                  ├─caller
│                  │  └─<AST::Node[Symbol] ('mutate')>
│                  └─operands=[
│                     <AST::Node[Call]
│                     ├─caller
│                     │  └─<AST::Node[Uneval] (<AST::Node[Symbol] ('counter')>)>
│                     └─operands=[
│                        <AST::Node[Call]
│                        ├─caller
│                        │  └─<AST::Node[Symbol] ('+')>
│                        └─operands=[
│                           <AST::Node[Call]
│                           ├─caller
│                           │  └─<AST::Node[Symbol] ('eval')>
│                           └─operands=[
│                              <AST::Node[Uneval] (<AST::Node[Symbol] ('counter')>)>
│                             ]>
│                           <AST::Node[Numeric] (1)>
│                          ]>
│                       ]>
│                    ]>
│                  <AST::Node[Call]
│                  ├─caller
│                  │  └─<AST::Node[Symbol] ('if')>
│                  └─operands=[
│                     <AST::Node[Call]
│                     ├─caller
│                     │  └─<AST::Node[Symbol] ('>')>
│                     └─operands=[
│                        <AST::Node[Call]
│                        ├─caller
│                        │  └─<AST::Node[Symbol] ('eval')>
│                        └─operands=[
│                           <AST::Node[Uneval] (<AST::Node[Symbol] ('counter')>)>
│                          ]>
│                        <AST::Node[Numeric] (10)>
│                       ]>
│                     <AST::Node[Symbol] ('break')>
│                    ]>
│                  <AST::Node[Call]
│                  ├─caller
│                  │  └─<AST::Node[Symbol] ('eval')>
│                  └─operands=[
│                     <AST::Node[Uneval] (<AST::Node[Call]
│                                           ├─caller
│                                           │  └─<AST::Node[Symbol] ('do')>
│                                           └─operands=[
│                                              <AST::Node[Call]
│                                                 ├─caller
│                                                 │  └─<AST::Node[Symbol] ('puts')>
│                                                 └─operands=[
│                                                    <AST::Node[Symbol] ('counter')>
│                                                   ]>
│                                             ]>)>
│                    ]>
│                 ]>
│              ]>
│         <AST::Node[Call]
│            ├─caller
│            │  └─<AST::Node[Symbol] ('delete')>
│            └─operands=[
│               <AST::Node[Uneval] (<AST::Node[Symbol] ('counter')>)>
│              ]>
│        ]>
│
├─<AST::Node[Call]
│      ├─caller
│      │  └─<AST::Node[Symbol] ('->')>
│      └─operands=[
│         <AST::Node[Call]
│            ├─caller
│            │  └─<AST::Node[Symbol] ('+')>
│            └─operands=[
│               <AST::Node[Symbol] ('%1')>
│               <AST::Node[Symbol] ('%2')>
│               <AST::Node[Symbol] ('%3')>
│               <AST::Node[Symbol] ('%4')>
│              ]>
│        ]>
│
├─<AST::Node[Call]
│      ├─caller
│      │  └─<AST::Node[Symbol] ('_')>
│      └─operands=[
│         <AST::Node[Numeric] (5)>
│         <AST::Node[Numeric] (5)>
│         <AST::Node[Numeric] (5)>
│         <AST::Node[Numeric] (5)>
│        ]>
│
├─<AST::Node[Call]
│      ├─caller
│      │  └─<AST::Node[Symbol] ('puts')>
│      └─operands=[
│         <AST::Node[Call]
│            ├─caller
│            │  └─<AST::Node[Symbol] ('string')>
│            └─operands=[
│               <AST::Node[String] ('5*4 =')>
│               <AST::Node[Symbol] ('_')>
│              ]>
│        ]>

\end{Verbatim}

    Nodes are all nested appropriately and all they need now is
    to be traversed and reduced to what they evaluate to.

    \subsubsection{Scoping \& Tables}
    For this section, I'll disregard the script we made, as that only handles
    lexical analysis and parsing. Instead I shall use the normal \code{./execute}
    file that is meant for executing files normally, however I will set
    \code{DEBUG} to \code{True} in \code{/lispy/config.py}. This will give a verbose
    output from the \code{/lispy/visitor.py} file especially.

    Running the command ``\code{./execute testing.lispy}'' in the root of
    the repository, will execute the program with extra debug information.
    The debug information is long, especially as certain functions are called hundreds
    of times, producing a lot of output to STDERR. Due to the volume of debug
    information, I will only include certain relevant snippets here.

    Running the command normally without debug mode, gets the following output
    to STDOUT (and none to STDERR):

    \begin{Verbatim}
1
2
3
4
5
6
7
8
9
10
5*4 = 20
    \end{Verbatim}

    Which is exactly as expected. We count to 10 and add 5 together, four times.

\clearpage

    Now let's run it with debug mode. Let's have a look at the last thing it executes,
    the shorthand-syntax lambda:

    \begin{verbatim}
Searching current scopes for symbol:  %1
Current parent tables searching: ['<TABLE`_main`:0x0 [frozen]>',
'<TABLE`_short_lambda`:0x7f5fb95b72b0 [frozen]>', '<TABLE`_main`:0x0>',
'<TABLE`_short_lambda`:0x7f5fb95b72b0>', '<TABLE`_short_lambda`:0x7f5fb95b72b0>']
Looking at: <TABLE`_short_lambda`:0x7f5fb95b72b0>
With bound symbols: ['%1', '%2', '%3', '%4']
Matched symbol in table.
5
[!!] - Finished symbol search.

Searching current scopes for symbol:  %2
Current parent tables searching: ['<TABLE`_main`:0x0 [frozen]>',
'<TABLE`_short_lambda`:0x7f5fb95b72b0 [frozen]>', '<TABLE`_main`:0x0>',
'<TABLE`_short_lambda`:0x7f5fb95b72b0>', '<TABLE`_short_lambda`:0x7f5fb95b72b0>']
Looking at: <TABLE`_short_lambda`:0x7f5fb95b72b0>
With bound symbols: ['%1', '%2', '%3', '%4']
Matched symbol in table.
5
[!!] - Finished symbol search.

Searching current scopes for symbol:  %3
Current parent tables searching: ['<TABLE`_main`:0x0 [frozen]>',
'<TABLE`_short_lambda`:0x7f5fb95b72b0 [frozen]>', '<TABLE`_main`:0x0>',
'<TABLE`_short_lambda`:0x7f5fb95b72b0>', '<TABLE`_short_lambda`:0x7f5fb95b72b0>']
Looking at: <TABLE`_short_lambda`:0x7f5fb95b72b0>
With bound symbols: ['%1', '%2', '%3', '%4']
Matched symbol in table.
5
[!!] - Finished symbol search.

Searching current scopes for symbol:  %4
Current parent tables searching: ['<TABLE`_main`:0x0 [frozen]>',
'<TABLE`_short_lambda`:0x7f5fb95b72b0 [frozen]>', '<TABLE`_main`:0x0>',
'<TABLE`_short_lambda`:0x7f5fb95b72b0>', '<TABLE`_short_lambda`:0x7f5fb95b72b0>']
Looking at: <TABLE`_short_lambda`:0x7f5fb95b72b0>
With bound symbols: ['%1', '%2', '%3', '%4']
Matched symbol in table.
5
[!!] - Finished symbol search.

Visiting (`testing.lispy' branch: 4):
[puts] --- <STDOUT>: `5*4 = 20'
\end{verbatim}
\clearpage
    Here, the lambda has already been defined, and so has its symbol table,
    and all we're doing here is executing it. As you can see, every time
    it encounters a symbol, say `\code{\%2}', it begins a symbol table search.
    It lists off all possible tables it could be in, searching from most immediate
    to farthest scope. For each table it searches through, it lists off all
    symbols bound in that table and checks each one of them, if it doesn't
    find anything, it moves on to the next table, on level up from the innermost
    scope, until it runs out of tables. If it does run out of tables, naturally
    it will throw an unbound symbol exception, as there exists no bound value
    to that symbol in the scope.

  \subsection{Testing through Sample Code}
    One way to test whether the features of my language work, is to simply
    run a set of example programs of which each use a subset of the language's
    features, running them all will test if all my features work, all using the
    same interpreter, meaning all the features are compatible with each other
    as well.

    The \code{/samples} directory contains 14 example programs written
    in LISPY, here they are in alphabetical order:
    \begin{Verbatim}
atoms.lispy        deep.lispy       functions.lispy  lists.lispy
blocks.lispy       eval.lispy       ifact.lispy      strings.lispy
compose.lispy      factorial.lispy  integral.lispy
declarative.lispy  fizzbuzz.lispy   internal.lispy
    \end{Verbatim}

    Each file has been used to test certain features as I was developing them.
    Here is a list of what each file does / tests:

    \begin{itemize}
      \item \code{atoms.lispy} --- Shows usage and basic syntax for Atoms.
      \item \code{lists.py} --- Extensively demonstrating every in-built
      list manipulation method.
      \item \code{factorial.lispy} --- A simple recursive factorial calculator.
      \item \code{ifact.lispy} --- An iterative implementation of the factorial
      calculator, to contrast against the recursive solution in terms of speed.
      \item \code{blocks.lispy} --- Demonstrates the usage of code blocks, i.e.
      the \code{(do ...)} block.
      \item \code{deep.lispy} --- Tests depth of call table lookups by nesting
      lambdas, as I had previous issues with this.
      \item \code{declarative.lispy} --- Shows off some prelude functions, mostly
      things that belong to the functional/declarative paradigm of programming.
      \item \code{fizzbuzz.lispy} --- This is the classic FizzBuzz program.
      It runs through a range of numbers, and prints each one of them, however,
      when it encounters a multiple of 3, it prints ``Fizz'', on a multiple of
      5 it prints ``Buzz'' and when both are true (it's a multiple of 15)
      it prints ``FizzBuzz''. The file has two implementations, one slightly na\"ive one
      which converts one number at a time, and can be passed to a \code{(map ...)}
      function across a range; and another which is more generalised and uses
      an explicit loop.
      \item \code{integeral.lispy} --- Computes an integral of a given function,
      it uses the Riemann summation method for a finite value of $dx$ to solve
      an integral computationally. The integral function itself computes a
      finitely bound integral with bounds passed in as parameters, (implemented recursively).
      \item \code{strings.lispy} --- A file in which a list of operations possible on
      a string is each tested.
      \item \code{compose.lispy} --- This files tests the function composition macro
      implemented internally.
      \item \code{eval.lispy} --- Tests the \code{(eval ...)} macro. This demonstrates
      not only evaluation and unevaluation, but also how one can have nested unevaluations.
      \item \code{functions.lispy} -- Here, a number of functions are defined
      and executed, testing the three ways of declaring a function (definitions,
      lambdas, and short-hand lambdas), and runs them, proving scoping capability
      (such as remembering parent scopes of where the function was defined (called
      scope or variable capturing)) and how functions can call themselves and other functions.
      \item \code{internal.lispy} --- Demonstrates the behaviour of internal
      macros and how they differ from normal methods and macros.
    \end{itemize}
  \subsection{Testing in the REPL}
    The REPL provides an excellent way of quickly prototyping and debugging your
    programs, as it will recover on errors, and fix where you left off.
    I used the REPL mainly in debugging the language for testing how my
    implementation handles incorrect code, and what error messages they give.

    The number of possible errors is huge, so I'll just demonstrate a few from the
    error types that the implement has (\code{Execution Error, Parse Error, Syntax Error
    and Warnings}).

    \subsubsection{Interpreter Warnings}
      An example of a warning, is when the programmer tries to load
      the same file twice (requiring a module more than once).

      Take for example the prelude library, which always gets loaded anyway.
      What would happen if I typed \code{(require :prelude/prelude)}?

      \graph{err-warn}{Warning after requiring an already loaded module in
      the REPL.}

      The warning informs you that you're loading an already loaded module, and
      hence it will clash with itself.

    \subsubsection{Syntax Errors}
      The most common syntax error to get in a Lisp like language, is
      mismatched parentheses, take these examples:

      \graph{too-many-parens}{Having an excess amount of parentheses.}
      \graph{too-few-parens}{ Having too few closing parentheses.}
      \graph{mismatched-parens}{An arrangement of parentheses which does not make
      any sense.}

      The great thing about the first two error messages about having an
      \emph{unbalanced} amount of parentheses, is that the error messages
      tell you exactly how to fix the line of code, and where they need
      fixing, just by doing as the error messages say. The examples
      would work just fine, and as intended, if the error messages are followed.

      \graph{eof-string}{An unmatched closing quote.}

    \subsubsection{Parse Errors}
      An example of a parse error, is passing the incorrect amount of
      arguments to a macro, since macros are expanded at parsing time.

      \graph{parse-error}{We passed too many arguments to the `\code{m}' macro.}

    \subsubsection{Execution Errors}
      The majority of errors defined are indeed execution errors, here are some
      examples:

      \graph{unbound-sym}{Trying to get the value of a symbol which has
      not had a value assigned to it.}

      \graph{exec-args}{Passing an incorrect amount of arguments to
      a function.}

      Type checking is done to certain internal macros, such as the
      Subtraction macro, as it makes no sense to subtract anything other
      than numbers, or say subtracting a string from a number:

      \graph{type-check}{Trying to subtract the string ``hello'' from the number
      3 does not make sense.}

      \graph{index-err}{Trying to index a list giving an incorrect index type.}

      \graph{num-call}{Trying to make a call to something that is not
      a function, namely a the number 4 here.}

      There are many more errors defined, these were only a few of them.

\clearpage

\section{Appraisal}
  \subsection{Comparison against Objectives}
    The language in terms of its features, has grown greatly from its original
    objectives. This is, however, not to say that the language implemented had become
    unfaithful to its original design, it has just grown to become a superset
    of the language I originally laid out.

    The implementation's prelude, and any other libraries that others
    may make allow for the language to continue to grow and mold, this is
    again made especially easy since the introduction of macros to the language.

  \subsection{Future Improvements, Potential \& Ideas}
    Right now, there is one major improvement that the implementation needs,
    and that is performance.  Performance can be increased in a number of ways,
    however the most notable and obvious ways to do so is to:
    \begin{itemize}
      \item \textbf{Implementation Language} --- It was never really a good idea
      to write an \emph{interpreter} in an \emph{interpreted} language such as
      Python, if I were to rewrite the implementation in my free-time, I'd
      certainly do so in C/C++ or Rust. Of course, doing this adds the extra work
      of having to write a garbage collector amongst other things, as I can no
      longer rely on the Python implementation to do those sorts of things
      for me.
      \item \textbf{Interpretation Style} --- Right now, the language interpreter
      is an AST traversing interpreter, this means that it walks up and down
      the AST in order to execute it. A much more performant approach,
      and in fact how most major interpreted languages are implemented, is to
      compile the AST in to a series of bytecode instructions that are then
      executed by a virtual machine, mimicking how an actual computer works,
      and these virtual machines can be very fast, some even JIT-compile their
      instructions to machine-code on the fly, which can actually lead to speeds
      comparable to those of compiled languages.
      \item \textbf{Optimiser} --- This LISPY implementation performs absolutely
      no optimisation on the program before executing it, everything you tell
      LISPY to do, it will exactly.  In the future I might add something like
      constant-folding to the parser, so say the code \code{(+ 2 3)} just becomes
      \code{5} at compile-time, because there's no use in performing an addition
      at run-time when the value is constant and trivially deducible at compile-time.
      Another optimisation is short-circuiting conditionals evaluation, where
      we can skip doing certain code if we know it will never run anyway, or runs
      for no reason. Take for example the condition:
      \code{(and (= 3 5) (something) (else that takes too long))}.
      Three will never be equal to five, and thus testing if the other
      conditions are true as well is a total waste of time.
    \end{itemize}

    These are only a few of the optimisation I could integrate into my
    implementation. But, alas, I hadn't enough time nor resources to throw
    myself into building a fully-fledged Lisp interpreter with all the modern
    bells and whistles possible.

  \subsection{Users' Usage \& Feedback}
    TODO.

  \clearpage
  \let\Section\section
  \def\section*#1{\Section{#1}}

  \printbibliography

\end{document}
