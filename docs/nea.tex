%  -*- coding: utf-8 -*-
%!TeX spellcheck = en-GB,en-US

\documentclass{article}
\usepackage{fancyvrb}
\usepackage{verbatim}
\usepackage[utf8]{inputenc}
\usepackage{textcomp}
\usepackage{pmboxdraw}
\usepackage{varwidth}
\usepackage{url}
\usepackage[margin=4.00cm]{geometry}
\usepackage{pgfplots,pgfplotstable}
\usepackage{bold-extra}
\pgfplotsset{/pgf/number format/use comma,compat=newest}

\usepackage{enumerate}
\renewcommand{\labelitemi}{{\boldmath$\cdot$}}
\renewcommand{\labelitemii}{-}


\newcommand{\code}[1]{\texttt{#1}}
\newcommand{\multiline}[1]{\begin{center}
                        \begin{tabular}{l}
                          \begin{verbatim}
                           #1
                         \end{verbatim}
                        \end{tabular}
                        As a branch on a bigger syntax tree.
                      \end{center}}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{gensymb}
\usepackage{enumitem}
\usepackage{lipsum}
\usepackage{changepage}

\usepackage{minted}
\newenvironment{snippet}[1][python]
  {\VerbatimEnvironment
   \begin{minted}[mathescape,
     linenos,
     numbersep=5pt,
     gobble=2,
     frame=lines,
     framesep=2mm]{#1}}
  {\end{minted}}


\usepackage[
  autocite=superscript,
  backend=biber,
  style=numeric,
  citestyle=numeric,
  sorting=none,
  dateabbrev=false,
  uniquelist=false
]{biblatex}
\addbibresource{references.bib}

\usepackage{graphicx}
\usepackage{float}
\graphicspath{ {./} }

\renewenvironment{abstract}{
\begin{minipage}{0.95\textwidth}
  \rule{\textwidth}{1pt}}
  {\par\noindent\rule{\textwidth}{1pt}
\end{minipage}}
\makeatletter

\newcommand{\graph}[2]{
  \begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{#1}
    \caption{#2}
  \end{figure}
}

\newcommand{\under}[1]{\centerline{\footnotesize{#1}}}
\newcommand{\super}{\textsuperscript}
\newcommand{\numero}{N\super{\underline{o}}}
\newcommand{\etc}{{\&}ct.}
\newcommand\blfootnote[1]{%
  \begingroup
  \renewcommand\thefootnote{}\footnote{#1}%
  \addtocounter{footnote}{-1}%
  \endgroup
}

\begin{document}
  \pagenumbering{gobble}

  \title{Designing a Lisp Dialect and Interpreter\\AQA NEA Comp. Sci.}
  \author{Samuel F. D. Knutsen}
  \date{Last Compiled: \today}
  \maketitle
  \blfootnote{Beauchamps High School and Sixth Form,}
  \blfootnote{Centre \numero 16137,}
  \blfootnote{Candidate \numero 2484.}
  \clearpage

  \tableofcontents
  \clearpage
  \pagenumbering{arabic}

  \setlength{\parindent}{2em}
  \setlength{\parskip}{0.75em}

  \begin{abstract}
    \textsc{\textbf{Abstract}} ---
    Throughout this article I will be looking at use cases, motivations,
    implementation, documentation and usage of building an interpreter for
    our new Lisp Dialect.
    \vspace{-5pt}
  \end{abstract}

\section{Analysis}
  In this document I will be investigating the usage and implementation of
  a Lisp Interpreter of our own variety, i.e. the language will be inspired
  by the conventions and harbour the basic properties of a Lisp. These
  variations upon the language are called Lisp Dialect, one of which will be
  our own, which I will be implementing, and outline in this document.

  Designing and implementing a language are two distinct steps, but I need
  to consider the other while working on each of the steps. That's to say, that
  when designing the language, I need to consider what would be realistic in
  implementing the language afterwards. The features of a language are
  limited to what I will be able to actually write the code for, not
  allowing for ambiguity.  In the second stage, when implementing it, I also
  need to stay true to our original design, and not stray from what I
  originally intended, unless I had set unrealistic design goals.

  \subsection{Motivation}
    Programming languages are not only naturally very important for programming
    and programmers, but also a very fascinating topic, bridging the gap between
    humans and the metal.

    For the reason of interest alone, I already wanted to make a programming
    language.  I have made programming languages a few times before, one of them
    another interpreter meant to resemble Assembly code, meant for getting
    used to the concepts of Assembly and how a computer works, by also creating
    a website to visualise the internals of a computer (website: fam.knutsen.co).
    However the language serves not much purpose other than for education.

    My motivations for making a Lisp interpreter, however, are many.
    Lisp, for one, is quite a powerful language, and I mean that in the sense
    of how much you can do and manipulate the language and your program themselves.
    The major advantage of Lisp is indeed that it can be manipulated the very
    same way data can be manipulated, as the language is data itself.

    Lisp macros are really what sets Lisp apart. If I asked you to implement
    a while loop statement or an if statement in Python, could you do it?
    If you tried, you'd end up with an ugly mess of lambdas or perhaps even pieces
    of your program written inside strings that get parsed \emph{after} run time, and
    such.  In Lisp such a matter is trivial, thanks to its powerful macro system
    that is capable of building parts of the program in the program itself.

    \clearpage

    Speaking of the \code{while} loop, it is indeed very often implemented
    in the Lisp language itself, as opposed to being built into the interpreter.
    Here is my example, which is quite trivial, and will become more
    understandable, after having read this paper through:

    \begin{minted}{Lisp}
      (define macro (while c b)
        (iterate (do
          (unless (eval c) break)
          (eval b))))
    \end{minted}


    Another motivation is the potential that I may implement some features
    into the language that other Lisp dialects don't have, in addition, I can
    also control certain features that I don't want in the language, that
    other Lisp dialects must almost religiously include (for example
    confusing naming on \code{car}, \code{cdr} and such, when \code{head}
    and \code{tail} would suffice).

  \subsection{Research}
    To begin this project, a certain amount of research needs to be covered.
    First we need to familiarise ourselves with the general concept of what
    defines and encompasses a Lisp and what I'd like my dialect to be
    inspired by.

    \subsubsection{What makes a Lisp?}
    Lisp is a \emph{family} of scripting/programming languages belonging typically
    to the declarative/functional paradigm of programming languages. It is
    characterised by its heavy use of parentheses (`\code(' and `\code)').
    The reason for this, is because the entire
    syntax itself is also a data structure\autocite{milestones}.
    More on the specifics on the syntax in the design section.

    \subsubsection{Making our language executable}
    Essentially, this is how to make a programming language be evaluated and
    how it can become executable on a computer, by a computer.

    Almost every implementation of the most common programming languages start
    with what is known as a `lexer' or `tokeniser' that performs lexical analysis.
    As one might gather from the name, this stage in the
    implementation tokenises the program.
    Every program start of a just a long string of characters stored in a file, these
    individual characters aren't useful, we want to gather them up in to groups
    of characters, e.g the program ``\code{print("Hello World")}'' is understood by
    the computer as just a string of characters:
    \code{[`p', `r', `i', `n', `t', `(', `"', ...]}, this is not useful.
    We want to group the individual components of the language together.
    As such:

    \noindent\code{[[IDENTIFIER, `print'], [L\_PAREN, `('], [STRING, `Hello
    World'], [R\_PAREN, `)']]}

    After this we proceed to the parsing stage. This essentially forms an
    abstract syntax tree from the tokens. This let's us define some sort of
    separation and concept of nesting between statements,
    grouping them together in a logical order.\autocite{dragon}
    e.g.

    \[ \code{"2 + 3"} \Rightarrow \code{[[NUMERIC, '2'], [OP, '+'], [NUMERIC,
'3']]}  \]
    \centerline{becomes}
    \begin{center}
      \begin{tabular}{l}
        \begin{BVerbatim}
         ...
          │
          ├───BINARY_OP::PLUS
          │    │
          │    ├───INTEGER<2>
          │    └───INTEGER<3>
         ...
       \end{BVerbatim}
      \end{tabular}
      As a branch on a bigger syntax tree.
    \end{center}


    \centerline{Another example:}
    \[ \code{"-4 * 3"} \Rightarrow \code{[[OP, '-'], [NUMERIC, '4'], [OP, '*'],
[NUMERIC, '3']]}  \]
    \begin{center}
      \begin{tabular}{l}
        \begin{BVerbatim}
         ...
          │
          ├───BINARY_OP::MUL
          │    │
          │    ├───UNARY_OP::NEG
          │    │    └───INTEGER<4>
          │    └───INTEGER<3>
         ...
       \end{BVerbatim}
      \end{tabular}
    \end{center}


    Now that I have our syntax tree, I will have to implement some sort of a
    visitor/walker, in order to execute our instructions, usually I could start
    at the leaves of the tree and work ourself up, eventually evaluating the whole
    expression, having evaluated the sub expressions.

    Luckily for me, I needn't consider operator precedence as `operators' are
    just function names which can be called, which forces prefix/polish notation
    to be used, where precedence is implied by the order of the operators. Another
    pro of parsing Lisps is that the Abstract Syntax Tree (`AST' from here on)
    already has a lot of similarity to the language itself, over all parsing is
    much easier than it would be for some of today's more natural interpreted
    programming languages.


\section{Design of the language}
  There are many ways of implementing a Lisp, the earliest Lisps had a very
  limited amount of key/reserved words (about 4), which from the early versions
  of MacLisp and Common Lisp has increased to about a minimum of about 9 or 10.

  One of the most important and powerful features of Lisp is that its
  syntax is practically its own syntax tree.  These nested lists that the Lisp
  grammar is composed from are called s-expressions\autocite{sexp}, meaning
  ``symbolic-expressions''.
  \subsection{Symbolic Expressions}
    In many (but not all) Lisp implementations and dialects, one of the most
    fundamental datatypes is the cons cell.  The cons cell is a way of constructing
    trees, and thus lists alike.

    Lisp ASTs are essentially linked-lists, capable of containing other nested
    lists.  Take the list \code{(x y z)} for example, to represent this in the form
    of cons cells, we need to consider the fundamental cons function. The cons
    function takes in a fundamental piece of data (i.e. not a list, very often
    called an `atom', but that names ambiguous) and a pointer
    to the next cons cell, and to end the list, simply a NULL pointer (an empty
    list or NIL is often used in place of NULL).

    Hence, \code{(x y z)} becomes \code{(cons x (cons y (cons z NULL)))}.\\
    But this way of writing is verbose, so the preferred notation is the
    dot-pair notation,\\
    so, \code{(cons x (cons y (cons z NULL)))} is written as \code{(x . (y . (z . NULL)))}

    \clearpage
    A linked list as such is implemented very plainly in C:

    \begin{minted}[mathescape,
                   linenos,
                   numbersep=5pt,
                   gobble=2,
                   frame=lines,
                   framesep=2mm]{c}
    struct cons_cell {
      atom data;
      struct cons_cell *next;
    };

    typedef struct cons_cell *cons;

    // Thus, (x y z) is
    atom x = Symbol('x');
    atom y = Symbol('y');
    atom z = Symbol('z');

    cons make_cons() {
      cons parent = malloc(sizeof(struct cons_cell));
      parent->next = NULL;

      return parent;
    }

    cons list = make_cons();
    list->data = x;
    list->next = make_cons();
    list->next->data = y;
    list->next->next = make_cons();
    list->next->next->data = z;
    list->next->next->next = NULL;  // Same as (x . (y . (z . NULL)))
    \end{minted}

    Which can be visualised as:
    \graph{cons-cells}{Visual representation of cons cells / a linked list}


    However, I will not be implementing my s-exps in this manner, nor
    will my Lisp dialect even contain the function \code{cons}
    (although \code{cons} can be very easily implemented as a macro).
    This is  because I will not be writing this in C, but rather in a higher
    level language (Python $>$3.7).  I'll be writing this in Python as it is
    what my school has available and what is taught there.

  \subsection{Syntax}
    Lisp consists of lists, and lists within lists,
    as I discussed earlier. Every list is denoted by a set of
    parentheses where the first element is generally a function name, and the
    following being its arguments. Some lists get expanded from their macro at
    the pre-compilation stage, and subsequently all lists get evaluated as a
    function call. One might wonder how you make a normal list, if Lisp treats
    all lists as function calls. Lisp has a somewhat unique feature, and that is
    the `unevaluate'/`quote' ``operator'' (if you will) which is denoted by
    putting
    a `\code{'}' (single single-quotation mark)
    in front of any expression, to stop the interpreter from evaluating it.
    So, your standard list or array is denoted:

    \begin{center}
      \code{'(a b c)}\\
      And when we want to return it back to its former unquoted self,\\
      we call the \code{eval} function on it:\\
      \code{(eval '(a b c))} is the same as \code{(a b c)}
    \end{center}

    Here we see that list items are separated by spaces, rather than the more
    common comma in other languages, making white-space significant to some
    degree. However, other than that, white-space is quite insignificant and
    the language has no need for things such as terminators (terminators are
    semi-colons in C and a new line in Python for example), and that's because
    everything is neatly wrapped up in parentheses and therefore every statement
    is automatically separated anyway.

    \subsubsection{Polish Notation / Prefix notation}
    Lisp could be said to be a language using Polish notation, that's to say
    that the language places its `operators' (which we'll discuss later) are
    prefix operators, or put in front.  Traditionally operators are placed in
    between their operands, this is called infix notation ($1 + 2$ for example).

    Prefix notation has some advantages and disadvantages, really, the only
    disadvantages to prefix notation is the fact that it may be less readable.
    The \emph{sole} reason for the fact that it is less readable is simply
    that we are not used to it, because we're used to infix notation.
    We could just as easily be using prefix notation, but things didn't turn out
    that way, but that doesn't prevent us from learning and quickly getting used
    to it.

    On the other hand, the advantages are make up for the disadvantages.
    One major advantage is the fact that precedence is implied, if and only if
    the exact amount of operands an operator can have are two; when that is true
    there is absolutely no need for things such as parentheses. However, Lisp
    \emph{does} make heavy use of parentheses and thus suffers from another
    formidable advantage, `n-arity' of operators. N-arity, Polyadic or Variadic
    functions/operators are simply operators that can take any number of operands.
    How would you add up five numbers together using infix notation?
    You'd have to use the operator four times over (e.g. $1 + 2 + 3 + 4 + 5$
    which bracketed would become $1 + (2 + (3 + (4 + 5)))$ (because really infix
    operators can only take two arguments)) as opposed to in Lisp, where you only
    need to use the $+$ operator once, (e.g. $(+\ 1\ 2\ 3\ 4\ 5)$, far fewer
    characters, use of operator was only once and no implicit brackets).

    The fact that Lisp uses prefix notation was not necessarily a choice for
    the language per se, but rather a consequence of the languages structure and
    a symptom of the language's aim to be itself \emph{data}.

    \subsubsection{Unique Data-types}
      Most Lisp dialects have about 7 fundamental datatypes:
      \begin{enumerate}
        \item \textsc{Integers}
        \item \textsc{Floats}
        \item \textsc{Cons}
        \item \textsc{Symbols} (including \textsc{Keywords})
        \item \textsc{Strings}
        \item \textsc{Functions}
        \item \textsc{Null}
      \end{enumerate}

      However as mentioned before, because of the way I'll be implementing this
      dialect, I will be excluding \code{cons} as a part of the language and will
      also be introducing a new one, which I'm calling the \textsc{Atom}.

      My new list of datatypes which will be implemented in my dialect is as
      following:

      \begin{itemize}
        \item \textsc{Numerics} (an umbrella for integers and floats)
        \item \textsc{Symbols}
        \item \textsc{Atoms}
        \item \textsc{Strings}
        \item \textsc{Defintions} (impure functions)
        \item \textsc{Nil}
      \end{itemize}

      My atoms are not at all your standard Lisp atom, in most Lisp contexts
      `atom' just means a fundamental datatype, but this is not what I'll be
      calling an atom.

      Atoms are a useful construct in my experience. The Ruby programming language
      has them, and calls the `Symbols', however that means something totally
      different in the context of a Lisp, the Erlang and Elixir programming
      languages have them, and many Lisps have something that heavily resembles
      atoms (both syntactically and practically) called `Keywords', but my `Atom'
      is slightly different.

      %% Extract from a file comment I made
      Atoms simply exist for their name.
      No atoms ever need to be created as
      one could simply imagine that all
      possible atoms already exist at once.

      When an atom is first used, it is assigned
      a location in memory based on some hash, and
      any future use of that same atom/atom-name will
      reference the same exact location in memory.

      Atoms are not strings, you can't change anything
      about them, to do that, simply use another atom.
      Two separate identical strings will take up and be
      declared in different locations in memory, this is
      not true for atoms. Atoms will always use the same
      location in memory, there is no reason for the to
      do otherwise, (unlike with strings)

      Atoms must start with a colon, then an identifier
      string, just like symbols (but with a colon in front).
      One may think of atoms as being used the same way
      as \code{enums} are, existing only for the purpose of
      having a way to identify something

    \subsubsection{Operators}
      Technically in Lisp, any function could be called an operator.
      It is also true that all the operators we traditionally call operators
      are also operators (i.e. \code{+, -, *, /, \%,} \etc).
      Hence, transitively, all (traditional) operators are functions!

      The fact that all operators are just functions, I see as a hugh advantage
      to the language, because, not only does that allow the programmer to
      extend the meaning of an operator, but it also allows them to create their
      own operators just as easily as defining any function. Another consequence
      of this that we may use operators in our function names / symbols.
      Just as \code{+} is a function, you could have a function named \code{++}
      or \code{***}, and that can be used with alphanumeric characters too, to
      have a function \code{*hello*} for example, and even \code{hello-world},
      which is a common mistake amongst beginner programers, trying to use hyphens
      in variables names, but the language (most likely C or Python, \etc),
      understands \code{hello-world} as \code{hello} \emph{minus} \code{world},
      as opposed to what the programmer \emph{actually} meant. Perhaps another
      advantage of Lisp\ldots

    \clearpage
    \subsubsection{Key/Reserved Words \& Symbols}
      Every language has a certain amount of reserved words and symbols that
      actually form, comprise and compose the syntax of a language, some
      languages will allow you to redefine these key words, but most won't, as
      it would essentially break the language.  Basic examples of these key
      words, in Python for example, are words such as: \code{return}, \code{while}
      \code{def} and \code{True} (also including operators such as \code{=}).

      My language should have a few defined reserved words, but my aim is to keep
      the number of them as low as possible, because, like in many Lisps, many
      things can be defined as macros in a standard library anyway.
      Some words that I plan to reserve are as follows:

      \begin{itemize}
        \item Yield statement, similar to a return, \code{(yield ...)}.
        \item Unevaluator, this is the single single-quote, \code{'some-name},
              \code{'(some list of symbols)}.
        \item Nil, this is the only kind of its type, and will be entirely
              reserved as its own lexeme, \code{nil}.
        \item All symbols such as \code{(} and \code{)}, and \code{"}, as they
              exist to describe other constructs.
      \end{itemize}

      Other than that, the semi-colons (\code{;}) will describe EON comments,
      Atoms and Symbols will be matched through certain expressions, and
      anything else will be completely ignored by the language.

      Other items, that are not understood by the Lexer and Parser, but are
      still technically part of the List of reserved words are the internal
      macros, which will be identified and dealt with at the evaluation stage
      are called `internal macros`, because their behaviour is technically like
      macros and they are built internally into the interpreter.

    \subsubsection{Internal Macros}
      Internal macros in a language are also not usually allowed to be reassigned
      in most languages, however a surprisingly large amount of Lisp dialects
      and implementations do allow this, however I will not be allowing this
      myself.


      \clearpage
      My initial list of internal macros will be as following, (but I am certain
      it will expand when I actually implement the language):

      \begin{itemize}
        \item \code{define} -- Probably the most important macro, it takes in a
                               name and arguments for a function, and creates a
                               function in the current scope with that name.
                               Essentially the equivalent of \code{def} in
                               languages such as Ruby, Python, Scala, \etc

        \item \code{lambda} -- Same as define, but the function is nameless.


        \item \code{let} -- Would bind a value to a symbol, say $x = 3$, in Lisp
                            we'd write \code{(let (x 3))}.

        \item \code{print} -- Print the operands of the print statement to STDOUT.
        \item \code{if} -- If is a thee way statement, the first operand is the
                           condition, the second is the consequence, the third
                           is the alternative, if-this-then-that-else-that.

        \item \code{<} -- Check if each operand is less than the next.
        \item \code{>} -- Check if each operand is greater than the next.
        \item \code{=} -- Check if all operands are the equal to each other.
        \item \code{/=} -- Check if at least one operand is not the same as the rest.

        \item \code{type} -- Returns the type of its evaluated operand.

        \item \code{list} -- Creates an unevaluated list, but all the operands
                             get evaluated at the construction if the list.



        \item \code{+} -- Addition macro will add all its operands.
        \item \code{-} -- Subtraction macro will subtract all its operands.
        \item \code{*} -- Multiplication macro will multiply all its operands.
        \item \code{/} -- Division macro will divide all its operands.
        \item \code{\%} -- Mod macro will give the remainder of the division of
                           all its operands.

        \item \code{eval} -- Takes an unevaluated list or string, and evaluates
                             it as normal code, a very important macro in any
                             Lisp dialect.
      \end{itemize}




  \subsection{Grammar}
    Any Lisp's grammar is naturally very similar, and for the most part quite
    simple, but has indeed steadily been increasing in complexity over the years.
    The Original Lisp had a very simple grammar and syntax, and had only the
    symbols \code{DEFINE}, \code{LAMBDA}, \code{LABEL}, \code{COND},
    \code{COMBINE}, \code{FIRST}, \code{REST}, \code{NULL}, \code{ATOM},
    \code{EQ}, \code{NIL}, \code{T}, and \code{QUOTE} predefined\autocite{sexp}.
    All symbol names were converted to upper- case and thus
    symbol names became case insensitive in early Lisps, this is
    still a tradition upheld in many Lisps today (because it's said to be less
    error prone), however this is not a tradition I will be upholding.

    Many modern Lisps have not only introduced the shortened quote syntax\\
    (\code{'...} as a shorting of \code{(quote ...)}), but have also introduced
    another form of quote, the `quasiquote'.

    The quasiquote works just as the normal quote but with added functionality.
    It can ``unquote'' its operands through use of the \code{(unquote ...)}
    macro inside of the \code{(quasiquote ...)} macro. This essentially means
    you can choose to evaluate certain operands, unlike quote, which keeps all
    operands unevaluated until you choose to evaluate them.

    In modern Lisps, some further shorting of syntax has been introduced:\\

    \begin{tabular}{lll}
        \code{(quasiquote ...)}       & has been aliased to& \code{`...} (a backquote)\\
        \code{(unquote datum)}        & has been aliased to& \code{,datum}\\
        \code{(unquote-splicing data)}& has been aliased to& \code{,@data}
    \end{tabular}

    {\small(Where \code{unquote-splicing} unquotes an entire list and merges it with
    the parent list)}

    Some examples of behaviour include:
    \begin{minted}[escapeinside=||,mathescape=true]{lisp}
    `(1 2  (+ 3 4) 5)      |$\Rightarrow$|    '(1 2 (+ 3 4) 5)
    `(1 2 ,(+ 3 4) 5)      |$\Rightarrow$|    '(1 2 7 5)
    `(1 2 ,@(list 3 4) 5)  |$\Rightarrow$|    '(1 2 3 4 5)
    \end{minted}

    Lisp syntax is always expanding, and the sharp-sign (\code{\#}), is actually
    a syntax element, purely meant for expanding the syntax itself, for example,
    in Common Lisp, \code{\#{\char`\\}c} is a character, \code{\#(...)} is a
    \emph{vector} and \code{\#xff} is the hexadecimal for $255_{10}$. At this point
    I personally think the syntax is getting out of hand, and I doubt I'll be
    implementing this in my Lisp dialect.

    \subsubsection{Generating a Token Stream}
      The lexical analyser is responsible for reading through our program string
      and splitting up the program in to sensible tokens representing a single
      datum, that comprises and represents something in the languages syntax.
      Take for example the program:\\``\code{(+ 12 (* 3 4))}'', our brains have
      already started tokenising and parsing the text, as soon as we look at it,
      and we can identify quickly some basic components, brackets, operators,
      numerics and spaces, but, for a computer these are all characters of no
      particular separation. A lexers job is to separate them in to parts, such
      as say\\\code{List [ Sym +, Num 12, List [ Sym *, Num 3, Num, 4 ] ]}, and
      ensure we dont get any non-sense tokens such as `\code{(+}' or `\code{* 3}'

      \clearpage

      Very often, when parsing regular languages, a lexical analyser may use
      simple regualar expressions to parse the language, say for example, a
      very primitive numeric matcher (for ints and floats), may look something
      like:

      \centerline{\code{/[0-9]+(\char`\\.[0-9]+)?/}}

      In pseudo-code, a basic lexer may look something like:

      \begin{minted}[linenos]{ruby}
      TokenStream = List

      lexer :: String -> TokenStream
      lexer (program) =>
        char_pointer = 0
        stream = new TokenStream

        partial = program[0..]

        while partial[0] != EOF =>
          condition if
            | (partial[0] == '(') =>
                stream.push (new Token L_PAREN)
                char_pointer = char_pointer + 1
            | (partial[0] == ')') =>
                stream.push (new Token R_PAREN)
                char_pointer = char_pointer + 1
            | (partial[0..2] == 'nil') =>
                stream.push (new Token NIL)
                char_pointer = char_pointer + 3
            | (match /[0-9]+(\.[0-9]+)?/ partial[0..]) =>
                stream.push (new Token NUMERIC matched)
                char_pointer = char_pointer + length(matched)
            | (match /[a-zA-Z_]+[a-zA-Z_0-9]*/ partial[0..]) =>
                stream.push (new Token IDENTIFIER matched)
                char_pointer = char_pointer + length(matched)
            | otherwise =>
                char_pointer = char_pointer + 1

          partial = program[char_pointer..]

        return stream
      \end{minted}

      \graph{lexer-flow}{The lexer also represented by a flow-chart.}

      This basic function \code{lexer} does most of the works, it takes in a
      \code{String} (the program string itself) and gives a \code{TokenStream}
      (which is really just an alias for a list). The lexer initialises some
      variables, very notably the \code{char\char`_pointer}, which very simply keeps
      a track of how many characters through the program code we are. Then the
      \code{partial} variable is just a section of the program string, it begins
      at the location of \code{char\char`_pointer} and all the way to the end.\\

      A while-loop will run all the way until we hit a null (\code{\char`\\0})
      EOF string terminator, and checks various conditions as well as incrementing
      the \code{char\char`_pointer} and updating the \code{partial} variable.
      The conditions check whether we've matched a certain token out of the beginning
      of the \code{partial} string, if so, we push a new \code{Token} to the
      \code{stream} list, with the appropriate type and matched string supplied.

    \subsubsection{Generating an Abstract Syntax Tree}
      Now that we have our list of tokens, the next step is to take those tokens
      and reconstruct the program as a tree.  The tree introduces important
      concepts back into the structure of the program.  Things such as nesting
      and precedence are clearly represented through the use of a tree.  A tree
      will also help us give appropriate scoping within certain nests at the
      evaluation stage.

      The parsing pattern we'll be implementing for generating our tree will
      likely be a bottom-up parser (or a shift-reduce parser). A shift-reduce
      parser does exactly what it says, it shifts tokens from the token stack
      to a temporary stack, and parses the \emph{leaves} of the tree, before it
      generates their branches (hence the name bottom-up).  It is often
      implemented recursively (as with many parsers) and is generally a lot more
      effective than a top-down parser, which requires a lot of guess-work.

      A general implementation of such a parser written in pseudo-code,
      could look something this:
      \clearpage
      \begin{minted}[linenos]{ruby}
# -- Get a our stream of tokens by calling the lexer
stream : TokenStream = Lexer::lex PROGRAM_STRING

# -- Define varius AST datatypes
DataType Tree =>
  self.children = []

DataType Call (values) =>
  self.values = values
  get self.operator =>
    return head self.values
  get self.operands =>
    return tail self.values

# --- Generalised datatype for an atomic AST datum
Abstract DataType Atomic (value) =>
  self.value = value

DataType Numeric inherits Atomic
DataType Symbol  inherits Atomic

# -- Actually implement the parser
parse :: TokenStream -> Tree  # (Type annotation)
parse (stream) =>
  tree = new Tree # Create an empty tree-root
  until empty? stream =>
    tree.children.push (atomic (stream)) # Deals with parsing individual datum
    stream.shift # Now shift off of the TokenStream stack
  return tree # Return the tree we've built up.

atomic :: TokenStream -> (Call | Atomic)
atomic (stream) =>
  condition if =>
    | (stream[0].type is L_PAREN) =>
        call = new Call []
        until stream[0] is R_PAREN =>
          stream.shift # shift off L_PAREN from stack (initially).
          call.values.push (atomic (stream)) # Push and recursively call
        return call                          # atomic on shifted stack.
    | (stream[0].type is NUMERIC) => return new Numeric (stream[0].string)
    | (stream[0].type is IDENTIFIER) => return new Symbol (stream[0].string)
    | otherwise =>
        Throw UnknownTokenType,
          "Token type" ++ stream[0].type ++ "is unknown."
        # If our lexer was implemented properly
        #   we wouldn't have to throw  an error, so we'll hopefully
        #   never reach the `Throw`.

      \end{minted}
      \clearpage

      The above code, may seem weird at first glance, so let's go through
      both of the functions defined above.

      \code{parse} --- The function \code{parse}, takes in a stream variable of
      type \code{TokenStream} and returns a tree of type \code{Tree} (which is
      basically a wrapper for a list).  Somewhat ironically, it doesn't implement
      any of the parsing algorithm itself, but rather calls a delegated
      function (\code{atomic}) to handle the parsing of atomic data. \code{parse}
      will run a loop, which runs until the stream of tokens has been completely
      emptied. In the loop body, we call the delegated \code{atomic} function on
      the stream, whose return value will get pushed as a child/branch onto the
      parent/root-tree. After that we shift one token off the stack and repeat
      until all tokens have been shifted off, (note: \code{atomic} may shift
      tokens off the stack as well, when parsing sub-expressions to a bigger
      expression).

      \code{atomic} --- The \code{atomic} function takes (probably incomplete,
      due to shifting) token \code{stream} variable of
      type \code{TokenStream} and returns either a \code{Call} node or a derived
      \code{Atomic} node such as \code{Numeric} or \code{Symbol} (i.e.
      \code{(Call | Atomic)}). \code{atomic}, is just one big conditional, that
      operates depending on which token happens to be on top of the stack at that
      point in time.  The first condition checks whether we have a function call
      being starting in our code (i.e. we've seen a left-parenthesis).

      \begin{adjustwidth}{1cm}{}
      \indent \indent If we do spot an \code{L\_PAREN}
      atop of our stack, we do indeed have a function call,
      and hence we must continue shifting off the stack until we reach a closing
      right-parenthesis.  But, remember, being a bottom-up parser, we must first
      parse all sub-expressions, so we recursively call \code{atomic}, to deal
      with all the expressions contained within the parentheses. Doing this, we
      also have implemented and permitted nesting into our language, by allowing
      other function calls to be parsed within parent function calls. When we've
      finally reached that corresponding right-parenthesis, and have pushed all
      our parsed sub-expression to the function call, we return the \code{Call}
      node and we end up back in the \code{parse} function.
      \end{adjustwidth}

      \clearpage

      \subsubsection{Macro Expansion Phase}
      After we've generated the most basic syntax tree, we should start the macro
      expansion phase.  Macros, you may be familiar with from other languages
      (especially compiled languages), they're part of the preprocessor stage,
      and thus don't do anything at evaluation time.

      Let's demonstrate through an example. Take the macro, and the function:

      \begin{minted}[escapeinside=||,mathescape=true]{lisp}
        (define macro  |\,|  (add-3-macro x) (+ x 3))
        (define function (add-3-func  x) (+ x 3))

        (print (add-3-macro 2) "\n") |$\Rightarrow$| 5
        (print (add-3-func  2) "\n") |$\Rightarrow$| 5
      \end{minted}

      Both \emph{eventually} end up evaluating to \code{5}, but did so by different
      methods. \code{add-3-func} had it's own Symbol Table made, that symbol table
      was pushed to the call stack, and the symbol \code{x} was bound in it.
      \code{x} was bound to the value \code{2} when the function was called, the
      function returned the evaluation of its body, with it's own bound symbols
      and then had it's symbol table wiped clean, and popped of the interpreter's
      call stack. Wew! All that for something that could have just been written
      as \code{(+ 2 3)}\ldots Well, that's exactly what that macro did!

      The macro, is essentially just text replacement (although what the macro
      accomplishes is actually done through manipulating the syntax tree).
      So, when the macro-expander sees your making a call with a macro-name as the
      caller, it essentailly copy-pastes the arguments into the macro body, and
      replaces all instances of \code{x} with \code{2} (in this example).

      So essentailly,
      \begin{minted}[escapeinside=||,mathescape=true]{lisp}
        (print (add-3-macro 2) "\n") |$\Rightarrow$| (print (+ 2 3) "\n")
      \end{minted}

  \subsection{Interpreter}
    Finally, to what's probably the most important part of the implementation.
    The interpreter or \emph{evaluator} is what will be doing the computation,
    by traversing the syntax tree that we've just generated.

    The interpreter will essentially consist of three important parts/aspects.
    \begin{enumerate}
      \item Symbol Tables, and using them for Scoping.
      \item Defining internal macros/functions.
      \item Running through the sub-trees of the AST recursively and evaluating
      appropriately certain expressions, by first evaluating the leaves of the
      tree, slightly similar to how we parsed the program.
    \end{enumerate}

    \clearpage

    \subsubsection{Scoping \& Binding Symbol in Symbol Tables}
    Let's start off by defining various Symbol Table stacks (such as call stacks
    and \emph{frozen} stacks and a list of current scopes)

\begin{minted}[linenos]{ruby}
DataType SymbolTable (scope_id) =>
  self.id = scope_id # A unique ID for each scope.
  self.local = %{} # A hash-table of all local variables to this scope.
  self.frozen = :false # Whether the symbol table can be modified
                       # (lambdas will have frozen super-scopes).
  self.bind (symbol, value) =>
    self.local[symbol] = value

  self.clean =>
    self.local = {} # Empty the local variables

# --- Define all the scope stack we need to keep track of:
ALL_SCOPES    = new Stack of SymbolTable
PARENT_SCOPES = new Stack of Integer

CALL_STACK    = new Stack of SymbolTable
GLOBAL_FROZEN = new Stack of (frozen)->SymbolTable
# ---

# A type for function definitions.
DataType FuncDef (scope, name, args, subtree) =>
  self.name = name
  self.subtree = subtree
  self.scope = scope
  self.table = find(scope, :id, ALL_SCOPES)
  self.args = args

  self.call (operands) =>  # What happens when we call a
    PARENT_SCOPES.push (self.scope) #    user defined function.
    CALL_STACK.push (self.table) # Push appropriate scopes.
    for i in length (self.args) =>
      self.table.bind(self.args[i], operands[i]) # Bind arguments.

    evaluate(self.subtree) # Finally evaluate the body.

    PARENT_SCOPES.pop
    CALL_STACK.pop # Pop them off again.
    self.table.clean # Get rid of bindings again.
\end{minted}
      This outlines the tools we'd be using, for dealing with scoping things in
      our language.
      \clearpage

    \subsubsection{Dealing with Internal Macros}
      Internal macros are really important. They provide a way to interface with
      the computer directly (or in this case, it'd be the implementation
      language (i.e. Python)), which introduces very useful functionality into
      the language. Take for example, the add \code{`+'} function/internal macro.
      It is basically required that this is not implemented in the language we're
      making, itself, as it is one of the most basic operations we can perform,
      and there'd be no way of implementing it. Others include \code{define} and
      \code{-}. I've alredy run through numerous macros earlier in this paper.

      Say for example, our interpreter encounters a symbol within our defined
      list internal macros, perhaps layed out as a hash-map like so:
      \begin{minted}[escapeinside=||,mathescape=true]{ruby}
        INTERNALS = {
          :define => __DEFINITION_MACRO__,
          :+      => __ADDITION_MACRO__,
          :-      => __SUBTRACTION_MACRO__,

          |$\vdots$|

          etc.
        }
      \end{minted}

      where \code{\_\_DEFINITION\_MACRO\_\_} and such are functions that deal with
      the computation of that internal macro. So in the evaluation function,
      we'd deal with it something like this.
      \begin{minted}[mathescape]{ruby}
        # Inside the evaluation function:
        evaluate (node) =>
          # ...
          if typeof (node) is Symbol =>
            if node.name is in INTERNALS =>
              return INTERNALS[node.name]
            # Otherwise, look in the Symbol Tables
            return lookup_symbol (PARENT_SCOPES, node.name)
          # ...
          if typeof (node) is Call =>
            caller = evaluate (node.caller)
            if typeof (caller) is FuncDef =>
              return caller.call (caller.operands)
            if caller is INTERNAL_DEFINITION =>
              return caller (node)
            else =>
              Throw (UncallableCallerError, "Can't make call to this type...")
          # ... etc.
      \end{minted}
      \clearpage
      What would an internal representation of one of those macros look like?
      Let's take the \code{+} internal macro, perhaps it would look something
      like this (in pseudo-code):
      \begin{minted}{ruby}
        __ADDITION_MACRO__ (call_node) =>
          args = map(evaluate, call_node.operands) # Evaluate every operand first.
          sum = 0
          for arg in args =>
            if typeof (arg) is not Numeric =>
              Throw TypeError, arg ++ " is not a Numeric type."
            sum = sum + arg
          return sum
          # An internal iterative solution is much more efficient
          #   than a more idiomatic recursive solution.
      \end{minted}
      Really, theres nothing fancy going on here, it is simply a way to build
      the most basic components of the language, and interface with features
      (such as basic addition) of the implementation language.
    \subsubsection{Interpretation Pattern}
      As briefly seen above, there are two basic functions I plan to be
      implementing.  A \code{visit} function and the very critical \code{evaluate}
      function.

      Let's start by describing the \code{visit} function. It's a simple to
      implement, all it needs to do is visit each root branch of the \code{Tree}
      and evaluate each branch (which will subsequently evaluate all its children
      trees recursively) in a simple loop. It may be implemented as such:

      \begin{minted}[linenos]{ruby}
        visit (AST) =>
          for child in AST =>
            evaluate (child)
          # The most basic idea of the visitor, although it may
          #   do error handling an other things as well.
      \end{minted}

      Naturally, we must have an implementation of the evaluate function too.
      We had partially implemented the \code{evaluate} function above, but let's
      now complete that implementation.

      \clearpage

      \begin{minted}[linenos]{ruby}
evaluate (node) =>
  case typeof(node)
  when Numeric =>
    return literal_eval (node.value) # Doesn't really get evaluated, per se,
                                     # as its really an 'atomic' datum.
  when String =>
    return node # Already the most basic datatype.
  when Atom =>
    # if the atom already exists in memory, just return it.
    if node.value is in ATOMS_HASHMAP =>
      return ATOMS_HASHMAP[node.value]
    # Otherwise, add it to the hashmap, giving it a unique location
    #   in memory, which will be referenced whenever the same atom is
    #   used again.
    ATOMS_HASHMAP[node.value] = new Atomise(node.value)
    return ATOMS_HASHMAP[node.value]
  when Symbol =>
    if node.name is in INTERNALS =>
      return INTERNALS[node.name]
    return lookup_symbol (PARENT_SCOPES, node.name)
    #      ^^^^^^^^^^^^^ Simple to implement, obvious what it does.
  when Call =>
    caller = evaluate (node.caller)
    if typeof (caller) is FuncDef =>
      return caller.call (caller.operands) # Make call to user defined function.
    if caller is INTERNAL_DEFINITION =>
      return caller (node)
    else =>
      Throw (UncallableCallerError, "Can't make call to this type...")

  default =>
    Throw (UnknownTreeNode, "I do not recognise the type of data you've
                             passed to be evaluated.")
      \end{minted}

  \subsubsection{Including a REPL}
    It'd be nice with a REPL as well. Quite simply a REPL is a
    READ-EVAL-PRINT-LOOP, and its implementation is in its name.
    \begin{minted}{lisp}
      (loop (print (eval (read))))
    \end{minted}
    This is quite a na\"{i}ve implementation, but it still works just fine,
    all we need to do, is execute the code.

    \clearpage
  \subsection{General Planned Implementation}
    We need to set out our files and file structure.


\section{Technical Solution \& Implementation}
  \subsection{Lexerical Analysis \& Tokenisation}
    \subsubsection{Matching through Regular Expressions}
  \subsection{Parser}
    \subsubsection{Bottom-up / Shift-reduce pattern}
    \subsubsection{Preprocessing \& Macro Expansion phase}
  \subsection{Interpreter/Evaluator}
    \subsubsection{Recursive decent evaluation}
    \subsubsection{Visiting \& Walking the tree}
    \subsubsection{Loading external files / Require statement}
  \subsection{All Internal Macros}
  \subsection{Debugging / Verbose Mode}
  \subsection{Prelude / Standard Library for the language}
  \subsection{Implementing a REPL}

\section{Documentation / Language Specification \& User Guide}

\section{Testing of Implementation}
  \subsection{Testing Discrete Sub-Components}
    \subsubsection{Lexical Analysis}
    \subsubsection{Initial Parse Tree}
    \subsubsection{Macro Expanded Tree}
    \subsubsection{Scoping \& Tables}
  \subsection{Testing the Prelude Library}
  \subsection{Testing through Sample Code}
  \subsection{Testing in the REPL}

\section{Appraisal}
  \subsection{Comparison against Objectives}
  \subsection{Future Improvements, Potential and Ideas}
  \subsection{Users' Usage and Feedback }


  \clearpage
  \let\Section\section
  \def\section*#1{\Section{#1}}

  \printbibliography

\end{document}
